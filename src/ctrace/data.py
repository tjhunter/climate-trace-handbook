"""
The data loading facilities for the ctrace package.

The main functions are `read_country_emissions` and `read_source_emissions`.
"""

import logging
import tempfile
from pathlib import Path
from typing import List, Optional, Tuple, TypeVar, Union
from zipfile import ZipFile

import huggingface_hub  # type: ignore
import polars as pl
import pooch  # type: ignore
from polars import col as C

from .constants import *
from .enums import *

_logger = logging.getLogger(__name__)

# A union type for the polars dataframes
Frame = TypeVar("Frame", pl.DataFrame, pl.LazyFrame)

# The archive files released by V3 of the Climate TRACE project
# TODO: this is only CO2E_100YR, add the other gas later.
# TODO: ask CT to provide the sha256 checksums directly, it is annoying to precalculate myself and less secure.
_files = {
    "agriculture.zip": "sha256:34b9f408dd18d9d3dcad695923619fef7d49d7296c3e95c8db11e3769ebae40b",
    "buildings.zip": "sha256:1399aba2de0a52db502d2a226810a288f5706bde5c11ee982c13bf6e57eea110",
    "fluorinated_gases.zip": "sha256:58478f4bc81fe7d61e93f915179b873c1199cfdcf12e85bd4f5e3070f99d6988",
    "forestry_and_land_use.zip": "sha256:40894910c3e36f19c7155e81a6778f4cc01ab8cdefabdb3f1a80dac95ae45dd1",
    "fossil_fuel_operations.zip": "sha256:b23575c07f5fbf89521dfb4f279c74b7017d5e6269a55c842254b61b8d3ad6d0",
    "manufacturing.zip": "sha256:0b61b46cbab6827bc702f8013c96e8d726acda632e050c250ae5fa1d7eb71869",
    "mineral_extraction.zip": "sha256:cefbc918a83794e64dbef7342fc0767c18e9b9011b418dae0a7e6eadb60cce69",
    "power.zip": "sha256:cb93d664b4d3f07e8d73fbee04db09a337ee0dfd5972d57c399b7ffff89993c2",
    "transportation.zip": "sha256:4a19ce826056bed9b3a4fc6e09f212087ac4f65ef00c01c48e84f261bc9a7a31",
    "waste.zip": "sha256:1aa3e32e83af8afebae8c8d56a331b22cd2bad06433d02b5515e8621c2afa5d4",
}

# The version of the dataset
# The first version "v2" is the official release from Climate TRACE.
# The second (year) is the release year.
# The third (ct) is the version of this dataset as generated by the ctrace package.
version = "v3-2024-ct3"
# The years covered by the dataset.
years = list(range(2021, 2025))

_ct_dset = pooch.create(
    # TODO: eventually allow versioning of the dataset
    path=pooch.os_cache("climate_trace"),
    version="v3-2024",
    # TODO: open a ticket on how best to handle the different gases.
    base_url="https://downloads.climatetrace.org/v3/sector_packages/co2e_100yr/",
    # The registry specifies the files that can be fetched
    registry=_files,
)


def read_source_emissions(
    year: Union[int, List[int], None] = None, p: Optional[Path] = None
) -> pl.LazyFrame:
    """
    Read all the source emissions data from the given path, assuming
    the source emissions have already been compacted into parquet files.

    If no path is provided, the data is read from a pre-compacted file
    stored on the internet.

    If you want to build the parquet files directly, use the `load_sources_parquet`
    functions.

    The year is used to filter the data on a specific year.
    If None, all the data is read.

    The path points to the directory holding the parquet files.
    If None, the data is read from the default location.

    """
    ys = _check_year(year)
    assert year is not None, "Year must be specified for now"
    fname = "climate_trace-sources_{version}_{year}.parquet"
    if p is None:
        local_paths = [
            huggingface_hub.hf_hub_download(
                repo_id="tjhunter/climate-trace",
                filename=fname.format(year=year_, version=version),
                repo_type="dataset",
            )
            for year_ in ys
        ]
    else:
        local_paths = [
            Path(p) / fname.format(year=year_, version=version) for year_ in ys
        ]
    return pl.concat(
        [
            pl.scan_parquet(local_p).pipe(recast_parquet, conf=True)
            for local_p in local_paths
        ]
    )


def load_source_compact(p: Optional[Path] = None) -> Tuple[pl.LazyFrame, List[Path]]:
    """
    Reads the source emissions data from the given path and creates
    a compacted view in Polars.
    """
    # Polars still has some issues with memory, especially because we are
    # joining the confidence while scanning the data.
    # The current strategy is to read eagerly each subsector, write them
    # to parquet in temporary files and then reload the full dataframe lazily..
    # TODO: replace by a proper temp directory
    tmp_dir = Path(tempfile.gettempdir())
    data_files: List[Path] = []
    for fname in _files:
        (zf, _) = _get_zip(p, fname)
        source_names_l = [n for n in zf.namelist() if n.endswith("sources.csv")]
        # The zip files do not seem to have been created correctly and some
        # entries are duplicated.
        source_names = sorted(list(set(source_names_l)))
        _logger.debug(f"sources: {fname} -> {source_names}")
        for sname in source_names:
            _logger.debug(f"opening {fname} / {sname}")
            tmp_name = tmp_dir / sname.replace(".csv", ".parquet").replace("DATA/", "")

            c_name = sname.replace(
                "_emissions_sources.csv", "_emissions_sources_confidence.csv"
            )
            _logger.debug(f"opening {fname} / {sname} and {c_name}")
            df = _load_source_conf(zf.open(sname), zf.open(c_name))
            # Remove all the empty strings, this provides better statistics and
            # removes unnecessary string compression.
            df = df.with_columns(
                [
                    pl.when(pl.col(pl.Utf8).str.len_bytes() == 0)
                    .then(None)
                    .otherwise(pl.col(pl.Utf8))
                    .name.keep()
                ]
            )
            _logger.debug(f"writing {tmp_name}")
            # Making large groups because they will be broken into smaller
            # during the split by year.
            df.write_parquet(
                tmp_name,
                compression="zstd",
                statistics=True,
                row_group_size=2_000_000,
                use_pyarrow=True,
            )
            data_files.append(tmp_name)
            _logger.debug(f"wrote {tmp_name}")
    dfs: List[pl.LazyFrame] = []
    for tmp_name in data_files:
        _logger.debug(f"scan {tmp_name}")
        df_ = pl.scan_parquet(tmp_name)
        df_ = df_.pipe(recast_parquet, conf=True)
        dfs.append(df_)
    res_df: pl.LazyFrame = pl.concat(dfs)
    return res_df, data_files


def _load_csv(
    filter,
    cols: Optional[List[str]] = None,
    p: Optional[Path] = None,
) -> pl.DataFrame:
    """
    Returns a subset of the CSV files, all merged into a single dataframe.

    There is no interpretation of the column types, they are all strings.

    Data is loaded eagerly, which consumes a lot of memory.

    This is mostly useful for debugging purposes, not part of the official API.
    """
    dfs: List[pl.DataFrame] = []
    for fname in _files:
        (zf, _) = _get_zip(p, fname)
        source_names = [n for n in zf.namelist() if filter(fname, n)]
        _logger.debug(f"sources: {source_names}")
        for sname in source_names:
            _logger.debug(f"opening {fname} / {sname}")
            df = pl.read_csv(zf.open(sname), infer_schema_length=0)
            if cols is not None:
                df = df.select(cols)
            df = df.with_columns(
                pl.lit(fname.replace(".zip", "")).alias("zip_name"),
                pl.lit(sname.replace(".csv", "")).alias("file_name"),
            )
            dfs.append(df)
    # diagonal -> make a union of all columns (pandas-like behavior)
    res_df: pl.DataFrame = pl.concat(dfs, how="diagonal")
    return res_df


def _get_zip(p: Optional[Path], name: str) -> Tuple[ZipFile, Path]:
    if p is None:
        local_p = _ct_dset.fetch(name)
    else:
        local_p = p / name
    return (ZipFile(local_p), local_p)


def _load_source_conf(s_fp, c_fp) -> pl.DataFrame:
    s_df = _load_sources(s_fp)
    c_df = _load_source_confidence(c_fp)
    # Workaround: some confidence records are duplicated:
    c_df = c_df.group_by(START_TIME, END_TIME, ISO3_COUNTRY, SOURCE_ID).agg(
        pl.first("*")
    )
    return s_df.join(
        c_df.drop(["created_date", "modified_date", SECTOR, SUBSECTOR]),
        on=[START_TIME, END_TIME, ISO3_COUNTRY, SOURCE_ID, GAS],
        how="left",
    )


def _load_source_confidence(fp) -> pl.DataFrame:
    dates = [START_TIME, END_TIME, CREATED_DATE, MODIFIED_DATE]
    cf_cols = [
        SOURCE_TYPE,
        CAPACITY,
        CAPACITY_FACTOR,
        ACTIVITY,
        EMISSIONS_FACTOR,
        EMISSIONS_QUANTITY,
    ]
    # TODO: make it lazy
    df = pl.read_csv(fp.read(), infer_schema_length=0)  # .lazy()
    _logger.debug(f"columns: {df.columns}")
    sels = (
        [_parse_date(col_name) for col_name in dates]
        + [
            c_iso3_country.cast(iso3_enum).alias(ISO3_COUNTRY),
            c_source_id.cast(pl.UInt64).alias(SOURCE_ID),
            c_sector.cast(sector_enum).alias(SECTOR),
            c_subsector.cast(subsector_enum).alias(SUBSECTOR),
            c_gas.cast(gas_enum).alias(GAS),
        ]
        + [
            C(col_name)
            .cast(confidence_level_enum, strict=False)
            .alias("conf_" + col_name)
            for col_name in cf_cols
        ]
    )
    df = df.select(*sels)
    # For debugging
    return df  # .limit(1_000)


def _load_sources(fp) -> pl.DataFrame:
    dates = [START_TIME, END_TIME, CREATED_DATE, MODIFIED_DATE]
    uint64s = [SOURCE_ID]
    floats = [
        EMISSIONS_QUANTITY,
        EMISSIONS_FACTOR,
        CAPACITY,
        CAPACITY_FACTOR,
        ACTIVITY,
        LAT,
        LON,
    ]
    # TODO: make it lazy
    df = pl.read_csv(fp.read(), infer_schema_length=0)  # .lazy()
    num_other = 12
    check_cols = (
        [
            ACTIVITY,
            ACTIVITY_UNITS,
            EMISSIONS_QUANTITY,
            EMISSIONS_FACTOR,
            EMISSIONS_FACTOR_UNITS,
            CAPACITY_UNITS,
            CAPACITY,
            CAPACITY_FACTOR,
            GEOMETRY_REF,
            LAT,
            LON,
            ORIGINAL_INVENTORY_SECTOR,
        ]
        + [f"other{i}" for i in range(1, num_other + 1)]
        + [f"other{i}_def" for i in range(1, num_other + 1)]
    )
    for col_name in check_cols:
        if col_name not in df.columns:
            df = df.with_columns(
                pl.lit(None).cast(pl.String, strict=False).alias(col_name)
            )
    # Check that the columns match exactly
    s1 = set(df.columns)
    s2 = set(all_columns)
    assert s1 == s2, (
        s1 - s2,
        s2 - s1,
        list(zip(sorted(df.columns), sorted(all_columns))),
    )
    df = df.select(*all_columns)
    return (
        df.with_columns(
            # Only start_time and end_time are required
            *[_parse_date(col_name) for col_name in dates]
        )
        .with_columns(
            c_iso3_country.cast(iso3_enum).alias(ISO3_COUNTRY),
            c_gas.cast(gas_enum).alias(GAS),
            c_temporal_granularity.cast(temporal_granularity_enum).alias(
                TEMPORAL_GRANULARITY
            ),
            c_original_inventory_sector.cast(original_inventory_sector_enum).alias(
                ORIGINAL_INVENTORY_SECTOR
            ),
            c_sector.cast(sector_enum).alias(SECTOR),
            c_subsector.cast(subsector_enum).alias(SUBSECTOR),
        )
        .with_columns(
            *[
                C(col_name).cast(pl.Float64, strict=False).alias(col_name)
                for col_name in floats
            ]
        )
        .with_columns(
            *[C(col_name).cast(pl.UInt64).alias(col_name) for col_name in uint64s]
        )
        # This is for debugging the memory consumption, remove eventually
        .limit(1_000_000_000)
    )


def recast_parquet(df: Frame, conf: bool) -> Frame:
    """
    Takes a loaded polars dataframe and recasts the columns to the appropriate types.

    This information gets lost in the parquet format.
    """
    df = df.with_columns(
        c_iso3_country.cast(iso3_enum).alias(ISO3_COUNTRY),
        c_gas.cast(gas_enum, strict=False).alias(GAS),
        # c_original_inventory_sector.cast(original_inventory_sector_enum).alias(
        #     ORIGINAL_INVENTORY_SECTOR
        # ),
        c_temporal_granularity.cast(temporal_granularity_enum).alias(
            TEMPORAL_GRANULARITY
        ),
        c_subsector.cast(subsector_enum).alias(SUBSECTOR),
        c_sector.cast(sector_enum).alias(SECTOR),
    )
    if EMISSIONS_QUANTITY_UNITS in df.collect_schema().names():
        # There is no emissions quantity for the sources (it is all defined in metric tonnes).
        # This applies to the country emissions.
        # TODO: make it an enum? it as always tonnes it seems for countries
        df = df.with_columns(
            c_emissions_quantity_units.cast(pl.Categorical).alias(
                EMISSIONS_QUANTITY_UNITS
            )
        )
    if conf:
        cf_cols = [
            SOURCE_TYPE,
            CAPACITY,
            CAPACITY_FACTOR,
            ACTIVITY,
            EMISSIONS_FACTOR,
            EMISSIONS_QUANTITY,
        ]
        df = df.with_columns(
            *[
                C("conf_" + col_name)
                .cast(confidence_level_enum, strict=False)
                .alias("conf_" + col_name)
                for col_name in cf_cols
            ]
        )
    return df


def read_country_emissions(p: Optional[Path] = None) -> pl.DataFrame:
    """Read all the country emissions data from the given path."""
    # So fast there is no need to store a materialized version.
    dfs = []
    for fname in _files:
        (zf, local_p) = _get_zip(p, fname)
        _logger.debug(f"Opening path {fname} from {local_p}")
        source_names = [
            n for n in zf.namelist() if n.endswith("_country_emissions.csv")
        ]
        # TODO(V3) There seems to be duplicate entries (but not data) in the zip files.
        source_names = sorted(list(set(source_names)))
        _logger.debug(f"sources: {source_names}")
        for sname in source_names:
            _logger.debug(f"opening {fname} / {sname}")
            df = _load_country_emissions(zf.open(sname))
            df = df.pipe(recast_parquet, conf=False)
            dfs.append(df)
    res_df = pl.concat(dfs)
    return res_df


def _load_country_emissions(fp) -> pl.DataFrame:
    dates = [START_TIME, END_TIME, CREATED_DATE, MODIFIED_DATE]
    floats = [EMISSIONS_QUANTITY]
    df = pl.read_csv(fp.read(), infer_schema_length=0)
    all_columns = [
        "iso3_country",
        "start_time",
        "end_time",
        "gas",
        "sector",
        "subsector",
        "emissions_quantity",
        "emissions_quantity_units",
        "temporal_granularity",
        "created_date",
        "modified_date",
    ]
    # Check that the columns match exactly
    s1 = set(df.columns)
    s2 = set(all_columns)
    assert s1 == s2, (
        s1 - s2,
        s2 - s1,
        list(zip(sorted(df.columns), sorted(all_columns))),
    )
    df = df.select(*all_columns)
    return df.with_columns(
        # Only start_time and end_time are required
        *[_parse_date(col_name) for col_name in dates]
    ).with_columns(
        *[
            C(col_name).cast(pl.Float64, strict=False).alias(col_name)
            for col_name in floats
        ]
    )


def _check_year(y: Union[int, List[int], None]) -> List[int]:
    if y is None or y == []:
        y = years
    if isinstance(y, int):
        y = [y]
    for year in y:
        assert year in years, f"Year {year} not a valid year. Valid years are {years}"
    return y


def _parse_date(col_name: str) -> pl.Expr:
    return (
        pl.col(col_name)
        .str.to_datetime(
            strict=(col_name in {START_TIME, END_TIME}),
            format="%Y-%m-%d %H:%M:%S",
            time_unit="ms",
            time_zone="UTC",
        )
        .alias(col_name)
    )

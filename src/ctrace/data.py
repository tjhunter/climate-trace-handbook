"""
The data loading facilities for the ctrace package.

The main functions are `read_country_emissions` and `read_source_emissions`.
"""

import functools
import logging
import tempfile
from pathlib import Path
from typing import List, Optional, Tuple, TypeVar, Union
from zipfile import ZipFile

import huggingface_hub  # type: ignore
import polars as pl
import pooch  # type: ignore
from polars import col as C

from .constants import *
from .enums import *

_logger = logging.getLogger(__name__)

# A union type for the polars dataframes
Frame = TypeVar("Frame", pl.DataFrame, pl.LazyFrame)

# The archive files released by V3 of the Climate TRACE project
# TODO: this is only CO2E_100YR, add the other gas later.
# TODO: ask CT to provide the sha256 checksums directly, it is annoying to precalculate myself and less secure.
_files = {
    CO2: {
        "agriculture.zip": "sha256:7bd26e0be0f9aa335c9ff68759003f52047d7a60b3e47335b3f77cd732969774",
        "buildings.zip": "sha256:aa2a0a7b6864c9f569682b9ff2a086329ea90719a2ecc48150c6fbd83f8c9edc",
        "fluorinated_gases.zip": "sha256:b1c024513dbca1d427d2d4435718d11c215ec97e4aae10458caf09216ab5acc3",
        "forestry_and_land_use.zip": "sha256:24a8f808311a35b08bb28c9c4e4adff0cf7e86c905646d4efa1f914ecf73b127",
        "fossil_fuel_operations.zip": "sha256:91fb66f48dab5ae0824d822c845dbe76caa3375355c769dc335a70fc735e2073",
        "manufacturing.zip": "sha256:1aaf81c7d9a1932c640d437e12817c4291c6abcb5db95ee12d7b6ac89edf0d7d",
        "mineral_extraction.zip": "sha256:9ed00f890521ca3aa6985d8be01817427710641363541f32f4a0056b884d1489",
        "power.zip": "sha256:fabb53b3533d36c401b57ad4c2c70003eefae9e832537aeb6203958983b279e4",
        "transportation.zip": "sha256:8418ed9c25d4d193f5b64e64efcd7b78ac8ebb1a2373d67b8092377e0ca59322",
        "waste.zip": "sha256:feb5716e83ac3adfdf71e0728a82c904b28ebe28aa0dc24983ff56f70806907c",
    },
    CO2E_100YR: {
        "agriculture.zip": "sha256:34b9f408dd18d9d3dcad695923619fef7d49d7296c3e95c8db11e3769ebae40b",
        "buildings.zip": "sha256:1399aba2de0a52db502d2a226810a288f5706bde5c11ee982c13bf6e57eea110",
        "fluorinated_gases.zip": "sha256:58478f4bc81fe7d61e93f915179b873c1199cfdcf12e85bd4f5e3070f99d6988",
        "forestry_and_land_use.zip": "sha256:40894910c3e36f19c7155e81a6778f4cc01ab8cdefabdb3f1a80dac95ae45dd1",
        "fossil_fuel_operations.zip": "sha256:b23575c07f5fbf89521dfb4f279c74b7017d5e6269a55c842254b61b8d3ad6d0",
        "manufacturing.zip": "sha256:0b61b46cbab6827bc702f8013c96e8d726acda632e050c250ae5fa1d7eb71869",
        "mineral_extraction.zip": "sha256:cefbc918a83794e64dbef7342fc0767c18e9b9011b418dae0a7e6eadb60cce69",
        "power.zip": "sha256:cb93d664b4d3f07e8d73fbee04db09a337ee0dfd5972d57c399b7ffff89993c2",
        "transportation.zip": "sha256:4a19ce826056bed9b3a4fc6e09f212087ac4f65ef00c01c48e84f261bc9a7a31",
        "waste.zip": "sha256:1aa3e32e83af8afebae8c8d56a331b22cd2bad06433d02b5515e8621c2afa5d4",
    },
}

# The version of the dataset
# The first version "v2" is the official release from Climate TRACE.
# The second (year) is the release year.
# The third (ct) is the version of this dataset as generated by the ctrace package.
version = "v3-2024-ct4"
# The years covered by the dataset.
# The range could be larger but it will be extended based on interest.
years = list(range(2021, 2025))


def _create_pooch(gas: Gas) -> pooch.Pooch:
    return pooch.create(
        # TODO: eventually allow versioning of the dataset
        path=pooch.os_cache(f"climate_trace_{gas}"),
        version="v3-2024",
        # TODO: open a ticket on how best to handle the different gases.
        base_url=f"https://downloads.climatetrace.org/v3/sector_packages/{gas}/",
        # The registry specifies the files that can be fetched
        registry=_files[gas],
    )


@functools.cache
def _ct_dset(gas: Gas) -> pooch.Pooch:
    return _create_pooch(gas)


def read_source_emissions(
    gas: Union[Gas, List[Gas]],
    year: Union[int, List[int], None] = None,
    p: Optional[Path] = None,
) -> pl.LazyFrame:
    """
    Read all the source emissions data from the given path, assuming
    the source emissions have already been compacted into parquet files.

    If no path is provided, the data is read from a pre-compacted file
    stored on the internet.

    If you want to build the parquet files directly, use the `load_sources_parquet`
    functions.

    The year is used to filter the data on a specific year.
    If None, all the data is read.

    The path points to the directory holding the parquet files.
    If None, the data is read from the default location.

    """
    ys = _check_year(year)
    gases = _check_gas(gas)
    fname = "climate_trace-sources_{version}_{year}_{gas}.parquet"
    if p is None:
        local_paths = [
            huggingface_hub.hf_hub_download(
                repo_id="tjhunter/climate-trace",
                filename=fname.format(year=year_, version=version, gas=gas),
                repo_type="dataset",
            )
            for year_ in ys
            for gas in gases
        ]
    else:
        local_paths = [
            Path(p) / fname.format(gas=gas, year=year_, version=version)
            for year_ in ys
            for gas in gases
        ]
    return pl.concat(
        [
            pl.scan_parquet(local_p).pipe(recast_parquet, conf=True)
            for local_p in local_paths
        ]
    )


def load_source_compact(p: Optional[Path] = None) -> Tuple[pl.LazyFrame, List[Path]]:
    """
    Reads the source emissions data from the given path and creates
    a compacted view in Polars.
    """
    # Polars still has some issues with memory, especially because we are
    # joining the confidence while scanning the data.
    # The current strategy is to read eagerly each subsector, write them
    # to parquet in temporary files and then reload the full dataframe lazily..
    # TODO: replace by a proper temp directory
    tmp_dir = Path(tempfile.gettempdir())
    data_files: List[Path] = []
    for gas in GAS_LIST:
        for fname in _files[gas]:
            _logger.debug(f"Opening path {fname} {gas}")
            (zf, _) = _get_zip(p, gas, fname)
            source_names_l = [n for n in zf.namelist() if n.endswith("sources.csv")]
            # The zip files do not seem to have been created correctly and some
            # entries are duplicated.
            source_names = sorted(set(source_names_l))
            _logger.debug(f"sources:{gas}: {fname} -> {source_names}")
            for sname in source_names:
                _logger.debug(f"opening {fname} / {sname}")
                tmp_name = (
                    tmp_dir
                    / gas
                    / sname.replace(".csv", ".parquet").replace("DATA/", "")
                )

                c_name = sname.replace(
                    "_emissions_sources.csv", "_emissions_sources_confidence.csv"
                )
                _logger.debug(f"opening {fname} / {sname} and {c_name}")
                df = _load_source_conf(zf.open(sname), zf.open(c_name))
                # Remove all the empty strings, this provides better statistics and
                # removes unnecessary string compression.
                df = df.with_columns(
                    [
                        pl.when(pl.col(pl.Utf8).str.len_bytes() == 0)
                        .then(None)
                        .otherwise(pl.col(pl.Utf8))
                        .name.keep()
                    ]
                )
                _logger.debug(f"writing {tmp_name}")
                # Create directories if they do not exist
                tmp_name.parent.mkdir(parents=True, exist_ok=True)
                # Making large groups because they will be broken into smaller
                # during the split by year.
                df.write_parquet(
                    tmp_name,
                    compression="zstd",
                    statistics=True,
                    row_group_size=2_000_000,
                    use_pyarrow=True,
                )
                data_files.append(tmp_name)
                _logger.debug(f"wrote {tmp_name}")
    dfs: List[pl.LazyFrame] = []
    for tmp_name in data_files:
        _logger.debug(f"scan {tmp_name}")
        df_ = pl.scan_parquet(tmp_name)
        df_ = df_.pipe(recast_parquet, conf=True)
        dfs.append(df_)
    res_df: pl.LazyFrame = pl.concat(dfs)
    return res_df, data_files


def _load_csv(
    filter,
    cols: Optional[List[str]] = None,
    p: Optional[Path] = None,
) -> pl.DataFrame:
    """
    Returns a subset of the CSV files, all merged into a single dataframe.

    There is no interpretation of the column types, they are all strings.

    Data is loaded eagerly, which consumes a lot of memory.

    This is mostly useful for debugging purposes, not part of the official API.
    """
    dfs: List[pl.DataFrame] = []
    for fname in _files:
        (zf, _) = _get_zip(p, "co2", fname)
        source_names = [n for n in zf.namelist() if filter(fname, n)]
        _logger.debug(f"sources: {source_names}")
        for sname in source_names:
            _logger.debug(f"opening {fname} / {sname}")
            df = pl.read_csv(zf.open(sname), infer_schema_length=0)
            if cols is not None:
                df = df.select(cols)
            df = df.with_columns(
                pl.lit(fname.replace(".zip", "")).alias("zip_name"),
                pl.lit(sname.replace(".csv", "")).alias("file_name"),
            )
            dfs.append(df)
    # diagonal -> make a union of all columns (pandas-like behavior)
    res_df: pl.DataFrame = pl.concat(dfs, how="diagonal")
    return res_df


def _get_zip(p: Optional[Path], gas: Gas, name: str) -> Tuple[ZipFile, Path]:
    if p is None:
        local_p = _ct_dset(gas).fetch(name)
    else:
        local_p = p / gas / name
    return (ZipFile(local_p), local_p)


def _load_source_conf(s_fp, c_fp) -> pl.DataFrame:
    s_df = _load_sources(s_fp)
    c_df = _load_source_confidence(c_fp)
    # Workaround: some confidence records are duplicated:
    c_df = (
        c_df.group_by(START_TIME, END_TIME, ISO3_COUNTRY, SOURCE_ID)
        .agg(pl.first("*"))
        .shrink_to_fit()
    )
    return s_df.join(
        c_df.drop(["created_date", "modified_date", SECTOR, SUBSECTOR]),
        on=[START_TIME, END_TIME, ISO3_COUNTRY, SOURCE_ID, GAS],
        how="left",
    ).shrink_to_fit()


def _load_source_confidence(fp) -> pl.DataFrame:
    dates = [START_TIME, END_TIME, CREATED_DATE, MODIFIED_DATE]
    cf_cols = [
        SOURCE_TYPE,
        CAPACITY,
        CAPACITY_FACTOR,
        ACTIVITY,
        EMISSIONS_FACTOR,
        EMISSIONS_QUANTITY,
    ]
    # Even with Polars, loading large CSV files is memory intensive.
    _logger.debug(f"loading source conf {fp}")
    # TODO: make it lazy
    df = pl.read_csv(
        fp.read(),
        has_header=True,
        infer_schema_length=0,
        infer_schema=False,
        try_parse_dates=False,
        rechunk=False,
        low_memory=True,
        batch_size=100_000,
    ).shrink_to_fit()  # .lazy()
    _logger.debug(f"columns: {df.columns}")
    sels = (
        [_parse_date(col_name) for col_name in dates]
        + [
            c_iso3_country.cast(iso3_enum).alias(ISO3_COUNTRY),
            c_source_id.cast(pl.UInt64).alias(SOURCE_ID),
            c_sector.cast(sector_enum).alias(SECTOR),
            c_subsector.cast(subsector_enum).alias(SUBSECTOR),
            c_gas.cast(gas_enum).alias(GAS),
        ]
        + [
            C(col_name)
            .cast(confidence_level_enum, strict=False)
            .alias("conf_" + col_name)
            for col_name in cf_cols
        ]
    )
    df = df.select(*sels).shrink_to_fit()
    _logger.debug("source conf: ")
    # For debugging
    return df  # .limit(1_000)


def _load_sources(fp) -> pl.DataFrame:
    dates = [START_TIME, END_TIME, CREATED_DATE, MODIFIED_DATE]
    uint64s = [SOURCE_ID]
    floats = [
        EMISSIONS_QUANTITY,
        EMISSIONS_FACTOR,
        CAPACITY,
        CAPACITY_FACTOR,
        ACTIVITY,
        LAT,
        LON,
    ]
    # Even with Polars, loading large CSV files is memory intensive.
    # The following options are used to reduce the memory footprint.
    # TODO: make it lazy
    _logger.debug(f"loading source {fp}")
    init_schema = {
        "source_id": pl.UInt64,
        "iso3_country": pl.Categorical,
        "gas": pl.Categorical,
        "sector": pl.Categorical,
        "subsector": pl.Categorical,
        "emissions_quantity": pl.Float64,
        "emissions_factor": pl.Float64,
        "capacity": pl.Float64,
        "capacity_factor": pl.Float64,
        "activity": pl.Float64,
        "lat": pl.Float64,
        "lon": pl.Float64,
    }
    df = pl.read_csv(
        fp.read(),
        has_header=True,
        infer_schema_length=0,
        infer_schema=False,
        schema_overrides=init_schema,
        try_parse_dates=False,
        low_memory=True,
        rechunk=False,
        batch_size=100_000,
    ).shrink_to_fit()
    num_other = 12
    check_cols = (
        [
            ACTIVITY,
            ACTIVITY_UNITS,
            EMISSIONS_QUANTITY,
            EMISSIONS_FACTOR,
            EMISSIONS_FACTOR_UNITS,
            CAPACITY_UNITS,
            CAPACITY,
            CAPACITY_FACTOR,
            GEOMETRY_REF,
            LAT,
            LON,
            ORIGINAL_INVENTORY_SECTOR,
        ]
        + [f"other{i}" for i in range(1, num_other + 1)]
        + [f"other{i}_def" for i in range(1, num_other + 1)]
    )
    for col_name in check_cols:
        if col_name not in df.columns:
            df = df.with_columns(
                pl.lit(None).cast(pl.String, strict=False).alias(col_name)
            )
    # Check that the columns match exactly
    # Some of the files have extra columns, we ignore them for now.
    s1 = set(df.columns)
    s2 = set(all_columns)
    assert s2.issubset(s1), (
        s1 - s2,
        s2 - s1,
        list(zip(sorted(df.columns), sorted(all_columns))),
    )
    df = df.select(*all_columns).shrink_to_fit()
    _logger.debug("loaded str")
    df = (
        df.with_columns(
            # Only start_time and end_time are required
            *[_parse_date(col_name) for col_name in dates]
        )
        .with_columns(
            c_iso3_country.cast(iso3_enum).alias(ISO3_COUNTRY),
            c_gas.cast(gas_enum).alias(GAS),
            c_temporal_granularity.cast(temporal_granularity_enum).alias(
                TEMPORAL_GRANULARITY
            ),
            c_original_inventory_sector.cast(original_inventory_sector_enum).alias(
                ORIGINAL_INVENTORY_SECTOR
            ),
            c_sector.cast(sector_enum).alias(SECTOR),
            c_subsector.cast(subsector_enum).alias(SUBSECTOR),
        )
        .with_columns(
            *[
                C(col_name).cast(pl.Float64, strict=False).alias(col_name)
                for col_name in floats
            ]
        )
        .with_columns(
            *[C(col_name).cast(pl.UInt64).alias(col_name) for col_name in uint64s]
        )
        # This is for debugging the memory consumption, remove eventually
        .limit(1_000_000_000)
        .shrink_to_fit()
    )
    _logger.debug("recast")
    return df


def recast_parquet(df: Frame, conf: bool) -> Frame:
    """
    Takes a loaded polars dataframe and recasts the columns to the appropriate types.

    This information gets lost in the parquet format.
    """
    df = df.with_columns(
        c_iso3_country.cast(iso3_enum).alias(ISO3_COUNTRY),
        c_gas.cast(gas_enum, strict=False).alias(GAS),
        # c_original_inventory_sector.cast(original_inventory_sector_enum).alias(
        #     ORIGINAL_INVENTORY_SECTOR
        # ),
        c_temporal_granularity.cast(temporal_granularity_enum).alias(
            TEMPORAL_GRANULARITY
        ),
        c_subsector.cast(subsector_enum).alias(SUBSECTOR),
        c_sector.cast(sector_enum).alias(SECTOR),
    )
    if EMISSIONS_QUANTITY_UNITS in df.collect_schema().names():
        # There is no emissions quantity for the sources (it is all defined in metric tonnes).
        # This applies to the country emissions.
        # TODO: make it an enum? it as always tonnes it seems for countries
        df = df.with_columns(
            c_emissions_quantity_units.cast(pl.Categorical).alias(
                EMISSIONS_QUANTITY_UNITS
            )
        )
    if conf:
        cf_cols = [
            SOURCE_TYPE,
            CAPACITY,
            CAPACITY_FACTOR,
            ACTIVITY,
            EMISSIONS_FACTOR,
            EMISSIONS_QUANTITY,
        ]
        df = df.with_columns(
            *[
                C("conf_" + col_name)
                .cast(confidence_level_enum, strict=False)
                .alias("conf_" + col_name)
                for col_name in cf_cols
            ]
        )
    return df


def read_country_emissions(gas: Gas, p: Optional[Path] = None) -> pl.DataFrame:
    """Read all the country emissions data from the given path."""
    # TODO: with V3 there is enough data that a materialized view is useful.
    dfs = []
    gases = _check_gas(gas)
    for gas_ in gases:
        for fname in _files[gas_]:
            (zf, local_p) = _get_zip(p, gas_, fname)
            _logger.debug(f"Opening path {fname} from {local_p}")
            source_names = [
                n for n in zf.namelist() if n.endswith("_country_emissions.csv")
            ]
            # TODO(V3) There seems to be duplicate entries (but not data) in the zip files.
            source_names = sorted(set(source_names))
            _logger.debug(f"sources: {source_names}")
            for sname in source_names:
                _logger.debug(f"opening {fname} / {sname}")
                df = _load_country_emissions(zf.open(sname))
                df = df.pipe(recast_parquet, conf=False)
                dfs.append(df)
    res_df = pl.concat(dfs)
    return res_df


def _load_country_emissions(fp) -> pl.DataFrame:
    dates = [START_TIME, END_TIME, CREATED_DATE, MODIFIED_DATE]
    floats = [EMISSIONS_QUANTITY]
    df = pl.read_csv(fp.read(), infer_schema_length=0)
    all_columns = [
        "iso3_country",
        "start_time",
        "end_time",
        "gas",
        "sector",
        "subsector",
        "emissions_quantity",
        "emissions_quantity_units",
        "temporal_granularity",
        "created_date",
        "modified_date",
    ]
    # Check that the columns match exactly
    s1 = set(df.columns)
    s2 = set(all_columns)
    assert s1 == s2, (
        s1 - s2,
        s2 - s1,
        list(zip(sorted(df.columns), sorted(all_columns))),
    )
    df = df.select(*all_columns)
    return df.with_columns(
        # Only start_time and end_time are required
        *[_parse_date(col_name) for col_name in dates]
    ).with_columns(
        *[
            C(col_name).cast(pl.Float64, strict=False).alias(col_name)
            for col_name in floats
        ]
    )


def _check_year(y: Union[int, List[int], None]) -> List[int]:
    if y is None or y == []:
        y = years
    if isinstance(y, int):
        y = [y]
    for year in y:
        assert year in years, f"Year {year} not a valid year. Valid years are {years}"
    return y


def _check_gas(g: Union[Gas, List[Gas]]) -> List[Gas]:
    if isinstance(g, str):
        g = [g]
    for gas in g:
        assert gas in GAS_LIST, f"Gas {gas} not a valid gas. Valid gases are {GAS_LIST}"
    return g


def _parse_date(col_name: str) -> pl.Expr:
    return (
        pl.col(col_name)
        .str.to_datetime(
            strict=(col_name in {START_TIME, END_TIME}),
            format="%Y-%m-%d %H:%M:%S",
            time_unit="ms",
            time_zone="UTC",
        )
        .alias(col_name)
    )

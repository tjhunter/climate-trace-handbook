{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion and formatting\n",
    "\n",
    "This notebook explains how to convert the Climate TRACE dataset to a format that is more appropriate for data science. \n",
    "\n",
    "```{note}\n",
    "This section is relevant for data engineers, or data scientists who want to understand how the data \n",
    "has been prepared. Skip if you just want to access the final, prepared data.\n",
    "```\n",
    "\n",
    "The original data from Climate TRACE is offered as a series of CSV files bundled in ZIP archives. That format is universally understood, but it is not the most effective for effective analysis with data science tools. In particular, it is large: the source data, uncompressed, is about 100GB for each gas! This is the size at which most people would consider this project to be \"big data\" or at least \"medium data\". With the proper choice of data storage, we will bring it down to a breezy \"small data\" without losing information along the way.\n",
    "\n",
    "Instead, we are going to use the Parquet format. This format has a number of advantages:\n",
    "- it is _column-based_ : data systems can process big chunks of data at once, rather than line by line. Also, depending on the information requested, systems will read only the relevant columns and skip the rest very effectively\n",
    "- it is _universal_ : most modern data systems will be able to read it.\n",
    "- it is _structured_ : basic information about numbers, categories, ... are preserved.\n",
    "\n",
    "\n",
    "Looking at the code, we are performing a few tricks:\n",
    "\n",
    "_Compacting the data_ We minimize the size of the files by taking advantage of its structures. In particular, we know in many cases that values are part of known enumerations (sectors, ...). We replace all these by `polars.Enumeration`s. Not only this makes files smaller, but it also allows data systems to make clever optimization for complex operations such as joining.\n",
    "\n",
    "_Lazy reading_ If we were to read all the source data using a traditional system such as Excel or Pandas, we would require a serious amount of memory. The files themselves are more than 5GB. Polars is capable of reading straight from the zip file in a streaming fashion. This is what Polars calls a Lazy dataframe, or LazyFrame. Even when doing complicated operations such as joining the source files with the confidence information, Polars only uses 3GB of memory on my machine. In fact, this way of working is so fast that the `ctrace` package directly reads all the country emissions data from the zip files in less than a second.\n",
    "\n",
    "_Using known enumerations_ You will see in the source code that nearly all the variables such as column names, names of gas and sectors, etc. are replaced CONSTANT_NAMES such as `CH4`,.... You can use that to autocomplete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from ctrace.constants import *\n",
    "import ctrace as ct\n",
    "import pyarrow\n",
    "from dds import data_function\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "import dds\n",
    "import huggingface_hub\n",
    "logging.getLogger(\"dds\").setLevel(logging.WARNING)\n",
    "dds.accept_module(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating optimized parquet files for source data\n",
    "\n",
    "This first section creates files that are the most effective for reading and querying. The general approach is as follows:\n",
    "\n",
    "1. Join the source and source confidence CSV files and writes them as parquet files for each subsector\n",
    "2. Aggregate by year into a yearly parquet file\n",
    "3. Optimize this parquet file for reading\n",
    "\n",
    "This first command creates parquet files that join the source and source confidences for each subsector, and returns a list of all the created files.\n",
    "\n",
    "In this notebook, another trick is to define the transformations as _data functions_. In short, this code will only run if the source code changes. This makes rerunning the notebooks very fast, and only updating when something has changed in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'agriculture.zip' from 'https://downloads.climatetrace.org/v3/sector_packages/co2/agriculture.zip' to '/home/tjhunter/.cache/climate_trace_co2/v3-2024'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     (_, files) \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload_source_compact()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n\u001b[0;32m----> 6\u001b[0m \u001b[43mload_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/dds/_annotations.py:68\u001b[0m, in \u001b[0;36mdata_function.<locals>.decorator_.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DDSException(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@data_function cannot be used with arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments were passed to the function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but this function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         DDSErrorCode\u001b[38;5;241m.\u001b[39mARG_IN_DATA_FUNCTION,\n\u001b[1;32m     67\u001b[0m     )\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_keep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/dds/_api.py:52\u001b[0m, in \u001b[0;36mkeep\u001b[0;34m(path, fun, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeep\u001b[39m(\n\u001b[1;32m     46\u001b[0m     path: Union[\u001b[38;5;28mstr\u001b[39m, DDSPath, pathlib\u001b[38;5;241m.\u001b[39mPath],\n\u001b[1;32m     47\u001b[0m     fun: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, _Out],\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     50\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _Out:\n\u001b[1;32m     51\u001b[0m     path \u001b[38;5;241m=\u001b[39m DDSPathUtils\u001b[38;5;241m.\u001b[39mcreate(path)\n\u001b[0;32m---> 52\u001b[0m     res: Optional[_Out] \u001b[38;5;241m=\u001b[39m \u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/dds/_api.py:199\u001b[0m, in \u001b[0;36m_eval\u001b[0;34m(fun, path, args, kwargs, dds_export_graph, dds_extra_debug, dds_stages)\u001b[0m\n\u001b[1;32m    195\u001b[0m extra_debug \u001b[38;5;241m=\u001b[39m dds_extra_debug \u001b[38;5;129;01mor\u001b[39;00m get_option(extra_debug_option)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _eval_ctx:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# Not in an evaluation context, create one and introspect\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eval_new_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_debug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/dds/_api.py:388\u001b[0m, in \u001b[0;36m_eval_new_ctx\u001b[0;34m(fun, path, args, kwargs, export_graph, extra_debug, stages)\u001b[0m\n\u001b[1;32m    382\u001b[0m kwargs_repr \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[1;32m    383\u001b[0m     [(key, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(arg))) \u001b[38;5;28;01mfor\u001b[39;00m (key, arg) \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    384\u001b[0m )\n\u001b[1;32m    385\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_new_ctx:Evaluating (eval) fun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_repr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_repr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    387\u001b[0m )\n\u001b[0;32m--> 388\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m _add_delta(t, ProcessingStage\u001b[38;5;241m.\u001b[39mEVAL)\n\u001b[1;32m    390\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_new_ctx:Evaluating (eval) fun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mload_sources\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@data_function\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data_sources\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sources\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     (_, files) \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_source_compact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m~/work/climate-trace-handbook/src/ctrace/data.py:193\u001b[0m, in \u001b[0;36mload_source_compact\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    189\u001b[0m c_name \u001b[38;5;241m=\u001b[39m sname\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_emissions_sources.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_emissions_sources_confidence.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    192\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopening \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43m_load_source_conf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Remove all the empty strings, this provides better statistics and\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# removes unnecessary string compression.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[1;32m    197\u001b[0m     [\n\u001b[1;32m    198\u001b[0m         pl\u001b[38;5;241m.\u001b[39mwhen(pl\u001b[38;5;241m.\u001b[39mcol(pl\u001b[38;5;241m.\u001b[39mUtf8)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen_bytes() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     ]\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m~/work/climate-trace-handbook/src/ctrace/data.py:271\u001b[0m, in \u001b[0;36m_load_source_conf\u001b[0;34m(s_fp, c_fp)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_source_conf\u001b[39m(s_fp, c_fp) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 271\u001b[0m     s_df \u001b[38;5;241m=\u001b[39m \u001b[43m_load_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_fp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     c_df \u001b[38;5;241m=\u001b[39m _load_source_confidence(c_fp)\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Workaround: some confidence records are duplicated:\u001b[39;00m\n",
      "File \u001b[0;32m~/work/climate-trace-handbook/src/ctrace/data.py:363\u001b[0m, in \u001b[0;36m_load_sources\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m    347\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading source \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    348\u001b[0m init_schema \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: pl\u001b[38;5;241m.\u001b[39mUInt64,\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso3_country\u001b[39m\u001b[38;5;124m\"\u001b[39m: pl\u001b[38;5;241m.\u001b[39mCategorical,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m: pl\u001b[38;5;241m.\u001b[39mFloat64,\n\u001b[1;32m    361\u001b[0m }\n\u001b[1;32m    362\u001b[0m df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m--> 363\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    364\u001b[0m     has_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    365\u001b[0m     infer_schema_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    366\u001b[0m     infer_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    367\u001b[0m     schema_overrides\u001b[38;5;241m=\u001b[39minit_schema,\n\u001b[1;32m    368\u001b[0m     try_parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m     low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m     rechunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    371\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100_000\u001b[39m,\n\u001b[1;32m    372\u001b[0m )\u001b[38;5;241m.\u001b[39mshrink_to_fit()\n\u001b[1;32m    373\u001b[0m num_other \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m    374\u001b[0m check_cols \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    375\u001b[0m     [\n\u001b[1;32m    376\u001b[0m         ACTIVITY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_def\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_other \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    391\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:916\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 916\u001b[0m         buf \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_N\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\n\u001b[1;32m    919\u001b[0m end \u001b[38;5;241m=\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1006\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1005\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1006\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@data_function(\"/data_sources\")\n",
    "def load_sources():\n",
    "    (_, files) = ct.data.load_source_compact()\n",
    "    return files\n",
    "\n",
    "load_sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with the loading, the data is partitioned by year. This is the most relevant for most users: most people are expected to look at specific years and sectors (especially the latest year). This reduces the amount of data to load.\n",
    "\n",
    "Let us have a quick peek at the data in one of these files. It looks already pretty good: a lot of the redundant data such as the enumerations has been deduplicated. All the enumeration data is now converted to integers, this is what `dictionary<values=string, indices=int32, ordered=0>` means. It is not quite ready for high performance however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.parquet import read_table\n",
    "fname = load_sources()[0]\n",
    "print(fname)\n",
    "read_table(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by year and optimizing the output\n",
    "\n",
    "The following block takes all the sector files and aggregates them by year. This is based on the expectation that most users will work on the latest year, and that some users will want to look into the trends across the years.\n",
    "\n",
    "Since these files will be read many times (every time we want to do a graph), it pays off to optimize them. The Parquet format is designed for fast reads of the relevant data. We will do two main optimizations: optimal compression, optimizing the row groups and adding statistics.\n",
    "\n",
    "\n",
    "\n",
    "_Compression_ Parquet allows some data to be compressed by columns. The first intuition is that, looking at each column of data separately, there will be more patterns and thus more opportunities to compress the data. The second intuition is that, in data-intensive application, reading the data is the bottleneck. It is then faster to read smaller compressed data in memory and then decompress it (losing a bit of time in compute), rather than reading larger, uncompressed data. Modern compression algorithms such as ZStandard or LZ4 are designed to be very effective at using a processor. Using them is essentially a pure gain in terms of processing speed.\n",
    "\n",
    "\n",
    "```{admonition} CTODO\n",
    "The year of a data record is defined by its start time. This may be different than the convention used by Climate Trace. To check.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_directory = \"/tmp\"\n",
    "years = ct.data.years\n",
    "version = ct.data.version\n",
    "gases = ct.constants.GAS_LIST\n",
    "\n",
    "@data_function(\"/write_data\")\n",
    "def write_data():\n",
    "    data_files = load_sources()\n",
    "    dfs = []\n",
    "    for tmp_name in data_files:\n",
    "        print(tmp_name)\n",
    "        df = pl.scan_parquet(tmp_name)\n",
    "        df = df.pipe(ct.data.recast_parquet, conf=True)\n",
    "        dfs.append(df)\n",
    "    ldf = pl.concat(dfs)\n",
    "    fnames = []\n",
    "    for gas in gases:\n",
    "        for year in years:\n",
    "            fname1 = f\"{write_directory}/pre_climate_trace-sources_{version}_{year}_{gas}.parquet\"\n",
    "            (\n",
    "                ldf.filter(c_start_time.dt.year() == int(year))\n",
    "                   .filter(c_gas == gas)\n",
    "                   .sort(by=[GAS, SECTOR, SUBSECTOR, ISO3_COUNTRY, SOURCE_ID])\n",
    "                   .sink_parquet(\n",
    "                    fname1,\n",
    "                    compression=\"zstd\",\n",
    "                    maintain_order=True,\n",
    "                    statistics=True,\n",
    "                )\n",
    "            )\n",
    "            fname = f\"{write_directory}/climate_trace-sources_{version}_{year}_{gas}.parquet\"\n",
    "            print(fname)\n",
    "            ds = pyarrow.dataset.dataset(fname1)\n",
    "            pyarrow.dataset.write_dataset(\n",
    "                ds,\n",
    "                base_dir=\"/tmp\",\n",
    "                basename_template=\"ds_{i}.parquet\",\n",
    "                format=\"parquet\",\n",
    "                partitioning=None,\n",
    "                min_rows_per_group=300_000,\n",
    "                max_rows_per_group=1_000_000,\n",
    "            )\n",
    "            shutil.copyfile(\"/tmp/ds_0.parquet\", fname)\n",
    "            fnames.append((fname1, fname))\n",
    "    return fnames\n",
    "\n",
    "write_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optimizing row groups_ A parquet file is a collection of groups of rows, and these rows are organized column-wise along with some statistics. We can choose how many groups to create: the minimum is one group (all the data into a single group), which is the most standard. This is not optimal however: reading can only be done by one processor core at a time. If we have more, they will sit idle. This is why it is better to choose the number of groups to be close to the expected number of processor cores (10-100). When reading, each core will process a different chunk of the file in parallel.\n",
    "\n",
    "Polars cannot do this yet, so the code below directly calls the `pyarrow` package to restructure the final file, calling the function `pyarrow.dataset.write_dataset`. \n",
    "\n",
    "Here is the parquet files produced directly by Polars. It is the result of joining datasets which themselves are the result of reading many files (each by subsector). It is very fragmented (see the `num_row_groups` statistics below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fname_pre, fname_post) = write_data()[0]\n",
    "print(fname_pre)\n",
    "print(fname_post)\n",
    "parquet_file = pyarrow.parquet.ParquetFile(fname_pre)\n",
    "# print(parquet_file.metadata.row_group(0).column(2).statistics)\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final file is more compact: only 58 row groups. It will be much faster to read (up to 50 times faster on my computer) because the readers do not need to gather information from each of the row groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pyarrow.parquet.ParquetFile(fname_post)\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Statistics_ Each row group in a parquet file has statistics. These statistics contain for each columns basic information such as minimum, maximum, etc. as you can see below. During a query, a data system first reads these statistics to check what blocks of data it should read. \n",
    "\n",
    "For example, the first row group only contains agriculture data (which you can infer from `min: agriculture` and `max: agriculture`). As the result, if a query is looking for waste data, it can safely skip this full block. \n",
    "\n",
    "Grouping the rows and creating statistics can dramatically reduce the amount of data being read and processed. Finding the right number of groups is a tradeoff between using more cores to read the data in parallel, and not having to read too many statistics descriptions. In the extreme case of the file created by Polars (5000 row groups), the statistics make up 40% of the file and can take up to 90% of the processing time! If your parquet file reads slowly, it is probably due to its internal layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pyarrow.parquet.ParquetFile(fname_post)\n",
    "parquet_file.metadata.row_group(0).column(12).statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know check that it works correctly. Let's load the newly created data instead of the default version stored on the internet, for the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ct.read_source_emissions(gas=CO2, year=2023, p=\"/tmp\")\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 15M records for this year. This is spread across multiple gas and also multiple trips in the case of boats or airplanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(pl.len()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of distinct source IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec = (sdf\n",
    ".group_by(SOURCE_ID, SECTOR)\n",
    ".agg(pl.len())\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of sources outside forestry and land use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec.filter(c_sector != FORESTRY_AND_LAND_USE).select(pl.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: no source is associated with multiple sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec.group_by(SOURCE_ID).agg(c_sector.n_unique()).filter(pl.col(SECTOR) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: no annual source should be duplicated by gas. It used to be the case with V2 release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf\n",
    ".filter(c_temporal_granularity ==\"annual\")\n",
    ".group_by(SOURCE_ID, GAS)\n",
    ".agg(pl.len())\n",
    ".filter(pl.col(\"len\") > 1)\n",
    ".sort(by=\"len\")\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: emissions should always be defined. V2 used to have empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ct.read_source_emissions(CO2E_100YR, 2023, \"/tmp\")\n",
    "(sdf\n",
    " .select(c_emissions_quantity.is_null().alias(\"null_emissions\"), c_subsector, c_iso3_country)\n",
    " .group_by(c_subsector, \"null_emissions\")\n",
    " .agg(pl.len())\n",
    " .collect()\n",
    " .pivot(index=SUBSECTOR, on=\"null_emissions\", values=\"len\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrity checks\n",
    "\n",
    "Before uploading and publishing data, it is a good idea to run a number of checks. Frameworks such as [pandera](https://pandera.readthedocs.io/en/latest/polars.html) are very helpful to implement these checks. Here we just check that Akrotiri and Dhekelia (country code XAD) is not included, as mentioned in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ct.read_source_emissions(gas=GAS_LIST, year=2022, p=\"/tmp\")\n",
    " .filter(c_iso3_country == \"XAD\")\n",
    " .select(pl.len())\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CO2e subsector data should be a superset of all sectors\n",
    "\n",
    "Here is an example of issue to investigate: one would expect the total CO2e_100yr (total emissions normalized by their CO2 equivalent) to be at least present for each sector in which emissions are reported. This is not the case for FLU, for instance for the `removals` subsector.\n",
    "\n",
    "```{admonition} CTODO\n",
    ":name: missing-co2e-subsectors\n",
    "Confirm with CT.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=20):\n",
    "    print(ct.read_source_emissions(gas=GAS_LIST, year=2022, p=\"/tmp\")\n",
    "     .group_by(c_sector, c_subsector, c_gas)\n",
    "     .agg(c_emissions_quantity.sum())\n",
    "     #.filter(c_emissions_quantity < 0)\n",
    "     .sort(by=[c_sector, c_subsector, c_gas])\n",
    "     .collect()\n",
    "     .pivot(GAS, index=[SECTOR, SUBSECTOR])\n",
    "     .filter(pl.col(CO2E_100YR).is_null())\n",
    "     .filter(pl.col(CO2) != 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parquet files for country emissions\n",
    "\n",
    "As of V3, country emission data is also large enough that it should be compacted in parquet files. Note the dramatic difference:\n",
    "\n",
    "- uncompressed CSV file: 106MB\n",
    "- compressed CSV file: 6MB\n",
    "- parquet: 1MB !!\n",
    "\n",
    "As highlighted, the parquet file also has the advantage of being very efficient at extracting only the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the official archives, read all the gases.\n",
    "\n",
    "@data_function(\"/read_country\")\n",
    "def read_country():\n",
    "    path = Path(tempfile.gettempdir()) / f\"climate-trace-countries-{ct.data.version}.parquet\"\n",
    "    print(path)\n",
    "    cdf = ct.read_country_emissions(ct.constants.GAS_LIST, archive_path=True)\n",
    "    # Optimizing to read by time and then gas.\n",
    "    # The logic being that country-specific files are already available from CT.\n",
    "    (cdf\n",
    "     .sort(by=[c_start_time,c_gas,c_iso3_country])\n",
    "      .write_parquet(path) # Not taking precautions, the file is so small.\n",
    "    )\n",
    "    return path\n",
    "\n",
    "p = read_country()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contry emissions: integrity checks\n",
    "\n",
    "In a production pipeline, before uploading the final data, we would run a number of checks again on the country emissions. Here are a few checks that we can run (and which are currently failing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = ct.read_country_emissions(parquet_path=p)\n",
    "cdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country emissions: CO2e data should be a superset of all country emissions\n",
    "\n",
    "We see that some subsectors are present in CO2 emissions but are missing in the aggregated CO2e emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=20):\n",
    "    print(cdf\n",
    "     .group_by(c_sector, c_subsector, c_gas)\n",
    "     .agg(c_emissions_quantity.sum())\n",
    "     .sort(by=[c_sector, c_subsector, c_gas])\n",
    "     .pivot(GAS, index=[SECTOR, SUBSECTOR])\n",
    "     .filter(pl.col(CO2E_100YR).is_null())\n",
    "     .filter(pl.col(CO2) != 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country emissions: some countries are excluded from the dataset\n",
    "\n",
    "The Climate TRACE documentation excludes certain countries from the final release, but they are still present in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_isos = [\"XAD\", \"XCL\", \"XPI\", \"XSP\"]\n",
    "(cdf\n",
    " .filter(c_iso3_country.is_in(excluded_isos))\n",
    " .group_by([ISO3_COUNTRY, c_start_time.dt.year(), GAS, SECTOR, SUBSECTOR])\n",
    " .agg(pl.len()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the Hugging Face Hub\n",
    "\n",
    "As a final step, we make the datasets available on Hugging Face as a downloadable dataset.\n",
    "\n",
    "This step will only work if you have the credentials to upload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub.utils\n",
    "upload = False\n",
    "if upload:\n",
    "    try:\n",
    "        api = huggingface_hub.HfApi()\n",
    "        for (_, fpath) in write_data():\n",
    "            fname = os.path.basename(fpath)\n",
    "            print(fname, fpath)\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=fpath,\n",
    "                path_in_repo=fname,\n",
    "                repo_id=\"tjhunter/climate-trace\",\n",
    "                repo_type=\"dataset\",\n",
    "            )\n",
    "        fpath = read_country()\n",
    "        fname = os.path.basename(fpath)\n",
    "        print(fname, fpath)\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=fname,\n",
    "            repo_id=\"tjhunter/climate-trace\",\n",
    "            repo_type=\"dataset\",\n",
    "        )\n",
    "    except huggingface_hub.utils.HfHubHTTPError as e:\n",
    "        print(\"error\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion and formatting\n",
    "\n",
    "This notebook explains how to convert the Climate TRACE dataset to a format that is more appropriate for data science. \n",
    "\n",
    "```{note}\n",
    "This section is relevant for data engineers, or data scientists who want to understand how the data \n",
    "has been prepared. Skip if you just want to access the final, prepared data.\n",
    "```\n",
    "\n",
    "The original data from Climate TRACE is offered as a series of CSV files bundled in ZIP archives. That format is universally understood, but it is not the most effective for effective analysis with data science tools. In particular, it is large: the source data, uncompressed, is about 100GB for each gas! This is the size at which most people would consider this project to be \"big data\" or at least \"medium data\". With the proper choice of data storage, we will bring it down to a breezy \"small data\" without losing information along the way.\n",
    "\n",
    "Instead, we are going to use the Parquet format. This format has a number of advantages:\n",
    "- it is _column-based_ : data systems can process big chunks of data at once, rather than line by line. Also, depending on the information requested, systems will read only the relevant columns and skip the rest very effectively\n",
    "- it is _universal_ : most modern data systems will be able to read it.\n",
    "- it is _structured_ : basic information about numbers, categories, ... are preserved.\n",
    "\n",
    "\n",
    "Looking at the code, we are performing a few tricks:\n",
    "\n",
    "_Compacting the data_ We minimize the size of the files by taking advantage of its structures. In particular, we know in many cases that values are part of known enumerations (sectors, ...). We replace all these by `polars.Enumeration`s. Not only this makes files smaller, but it also allows data systems to make clever optimization for complex operations such as joining.\n",
    "\n",
    "_Lazy reading_ If we were to read all the source data using a traditional system such as Excel or Pandas, we would require a serious amount of memory. The files themselves are more than 5GB. Polars is capable of reading straight from the zip file in a streaming fashion. This is what Polars calls a Lazy dataframe, or LazyFrame. Even when doing complicated operations such as joining the source files with the confidence information, Polars only uses 3GB of memory on my machine. In fact, this way of working is so fast that the `ctrace` package directly reads all the country emissions data from the zip files in less than a second.\n",
    "\n",
    "_Using known enumerations_ You will see in the source code that nearly all the variables such as column names, names of gas and sectors, etc. are replaced CONSTANT_NAMES such as `CH4`,.... You can use that to autocomplete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from ctrace.constants import *\n",
    "import ctrace as ct\n",
    "import pyarrow\n",
    "from dds import data_function\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "import dds\n",
    "import huggingface_hub\n",
    "logging.getLogger(\"dds\").setLevel(logging.WARNING)\n",
    "dds.accept_module(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "ct.data._ensure_duckdb()\n",
    "# # Install and load the spatial extension\n",
    "# duckdb.execute('INSTALL spatial')\n",
    "# duckdb.execute('LOAD spatial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sector = \"forestry-and-land-use\"\n",
    "# sector_points = \"manufacturing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gases = GAS_LIST\n",
    "polys_path = ct.data.extract_polygons(p=True,gases=gases)\n",
    "polys_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_path = ct.data.extract_points(p=True, gases=gases, polys=polys_path)\n",
    "points_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "fiona.listlayers(\"/tmp/co2/DATA/buildings_geometries.gpkg\")\n",
    "# points_path\n",
    "# pl.read_parquet(points_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq_points = ct.data._extract_points(p=True, polys=polys_path, gases=[\"co2\"])\n",
    "# pq_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (pl.scan_parquet([p_fname for (_, p_fname) in l])\n",
    "#  .group_by(\"geometry_ref\")\n",
    "#  .agg(pl.col(\"geom\").first())\n",
    "# .head(10)\n",
    "# .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: extract the polygons from the geometry files:\n",
    "\n",
    "The geometry information will be stored in WKB format (a current limitation of DuckDB's writing capabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpkg_fname = f\"/home/tjhunter/.cache/climate_trace_co2/v3-2024/{sector}_geometries.gpkg\"\n",
    "# gpkg_points_fname = f\"/home/tjhunter/.cache/climate_trace_co2/v3-2024/{sector_points}_geometries.gpkg\"\n",
    "# duckdb.sql(f\"\"\"\n",
    "# COPY(\n",
    "#     SELECT * FROM ST_read('{gpkg_fname}', layer='{sector}_polygons')\n",
    "# ) TO '/tmp/{sector}_polygons.parquet';\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_geometry_ref = pl.col(\"geometry_ref\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: enrich the geometries with administrative information\n",
    "\n",
    "Most of the geometries stored by Climate TRACE are administrative boundaries (countries, regions, cities). This is the base for partitioning all the datasets. There are not so many administrative boundaries, so we will sometimes just store the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_geometry_ref = pl.col(\"geometry_ref\")\n",
    "# def _enrich_subsector(pdf, sector):\n",
    "#     return (pdf\n",
    "#      .with_columns(pl.col(\"subsectors\")\n",
    "#               .str.strip_chars_start(\"{\")\n",
    "#               .str.strip_chars_end(\"}\")\n",
    "#               .str.split(by=\",\")\n",
    "#                .cast(pl.List(ct.enums.subsector_enum))\n",
    "#               .alias(\"subsectors\"),\n",
    "#      pl.lit(sector).cast(ct.enums.sector_enum).alias(SECTOR),\n",
    "#                   )\n",
    "#     )\n",
    "\n",
    "# def _enrich_gadm(pdf, col_ref):\n",
    "\n",
    "#     pdf = (pdf\n",
    "#            .with_columns(\n",
    "#     pl.when(col_ref.str.starts_with(\"gadm\"))\n",
    "#      .then(col_ref.str.count_matches(\".\",literal=True))\n",
    "#      .otherwise(None)\n",
    "#      .alias(\"gadm_level\")\n",
    "#  ).with_columns(col_ref\n",
    "#                 .str.strip_prefix(\"gadm_\")\n",
    "#                 .str.head(3)\n",
    "#      .cast(ct.enums.iso3_enum, strict=False)\n",
    "#      .alias(ISO3_COUNTRY))\n",
    "\n",
    "#           )\n",
    "#     return pdf\n",
    "\n",
    "# (pl.scan_parquet(f\"/tmp/{sector}_polygons.parquet\")\n",
    "#  .pipe(_enrich_gadm, col_ref=c_geometry_ref)\n",
    "#  .pipe(_enrich_subsector, sector=sector)\n",
    "# #.head(10)\n",
    "# #.drop(\"geom\")\n",
    "# # .collect()\n",
    "#  .sink_parquet(f\"/tmp/{sector}_polygons_gadm.parquet\")\n",
    "# #.filter(pl.col(\"geometry_ref\").str.contains(\"NLD.9\"))\n",
    "# #.group_by(\"subsectors\")\n",
    "# #.agg(pl.len())\n",
    "# )\n",
    "\n",
    "# #pdf_polys_gadm = (\n",
    "#     #pdf_polys\n",
    "#  #.pipe(_enrich_gadm)\n",
    "#  #.pipe(_enrich_subsector, sector=sector)    \n",
    "# #)\n",
    "# #pdf_polys_gadm.write_parquet(f\"/tmp/{sector}_polys_gadm.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: add the GADM mapping to the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pl.scan_parquet(f\"/tmp/{sector}_polygons_gadm.parquet\")\n",
    "# .drop(\"geom\")\n",
    "#.filter(c_geometry_ref.str.contains(\"gadm_ALA.2\"))\n",
    "#.head(10)\n",
    "#.collect()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pdf_points = \n",
    "# duckdb.sql(f\"\"\"\n",
    "# SELECT COUNT(*) FROM ST_read('{gpkg_points_fname}', layer='{sector_points}_points')\n",
    "# \"\"\").pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb.sql(f\"\"\"\n",
    "# SELECT *, ST_X(geom) as lng, ST_Y(geom) as lat, FROM ST_read('{gpkg_points_fname}', layer='{sector_points}_points')\n",
    "# LIMIT 4\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb.sql(f\"\"\"\n",
    "# SET memory_limit = '4GB';\n",
    "# CREATE OR REPLACE TABLE polys AS SELECT *, ST_GeomFromWKB(geom) AS geom2 FROM '/tmp/{sector}_polygons_gadm.parquet';\n",
    "# CREATE OR REPLACE TABLE points AS SELECT *, ST_X(geom) as lng, ST_Y(geom) as lat FROM ST_read('{gpkg_points_fname}', layer='{sector_points}_points');\n",
    "# COPY (\n",
    "#     SELECT \n",
    "#         points.geometry_ref,\n",
    "#         points.subsectors,\n",
    "#         polys.iso3_country,\n",
    "#         polys.geometry_ref AS gadm,\n",
    "#         polys.gadm_level,\n",
    "#         points.lat,\n",
    "#         points.lng,\n",
    "#         points.geom \n",
    "#     FROM points\n",
    "#     JOIN polys ON st_intersects(points.geom, polys.geom2)\n",
    "# ) TO '/tmp/{sector_points}_points.parquet';\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_gadm_level = pl.col(\"gadm_level\")\n",
    "# (pl.scan_parquet(f\"/tmp/{sector_points}_points.parquet\")\n",
    "#   .filter(c_gadm_level.is_null())\n",
    "# .select(pl.len())\n",
    "# .collect()\n",
    "# .item()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_gadm = pl.col(\"gadm\")\n",
    "\n",
    "# (pl.scan_parquet(f\"/tmp/{sector_points}_points.parquet\")\n",
    "#  .group_by(c_geometry_ref)\n",
    "#  .agg(c_gadm, pl.col(\"geom\").first(), pl.col(\"lat\").first(), pl.col(\"lng\").first())\n",
    "#  .with_columns(c_gadm.map_elements(lambda l: sorted(l, key=len), return_dtype=pl.List(pl.String)))\n",
    "#  .with_columns(\n",
    "#      c_gadm.list.get(0,null_on_oob=True).alias(\"gadm_0\"),\n",
    "#      c_gadm.list.get(1,null_on_oob=True).alias(\"gadm_1\"),\n",
    "#      c_gadm.list.get(2,null_on_oob=True).alias(\"gadm_2\"),\n",
    "#  ).with_columns(\n",
    "#      c_gadm.list.get(-1,null_on_oob=True).alias(\"gadm\"),\n",
    "#  ).pipe(_enrich_gadm, col_ref=c_gadm)\n",
    "# .collect()\n",
    "# .write_parquet(f\"/tmp/{sector_points}_points_gadm.parquet\")\n",
    "# #.sink_parquet(f\"/tmp/{sector}_points_gadm.parquet\")\n",
    "# )\n",
    "\n",
    "# #l = [\"gadm_PRK.5_1\", \"gadm_PRK\", \"gadm_PRK.5.1_1\"]\n",
    "# #sorted(l, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points_pdf = pl.read_parquet(points_path)\n",
    "all_geoms_pdf = pl.read_parquet(polys_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat,lng=52.1462902,4.3977197  #WAS\n",
    "lat,lng=50.8512465,4.3454161 # BRU\n",
    "lat,lng=53.0547292,4.7374957 # TEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from lonboard import viz\n",
    "\n",
    "\n",
    "\n",
    "poly_geoms = duckdb.sql(f\"\"\"\n",
    "SELECT geometry_ref, gadm_level, geom_wkb FROM\n",
    "(\n",
    "    SELECT *, ST_GeomFromWKB(geom_wkb) AS geom FROM all_geoms_pdf\n",
    "    WHERE ST_WITHIN(ST_POINT({lng},{lat}), geom)\n",
    "    AND gadm_level = 2\n",
    ")\n",
    "\"\"\").pl()\n",
    "poly_geoms_df = poly_geoms.to_pandas()\n",
    "poly_geoms_pdf = geopandas.GeoDataFrame(poly_geoms_df,\n",
    "                                        geometry=geopandas.GeoSeries.from_wkb(poly_geoms_df['geom_wkb']), crs=\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadms_ref = set(poly_geoms[GEOMETRY_REF].to_list())\n",
    "gadms_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz(poly_geoms_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_geoms = duckdb.sql(f\"\"\"\n",
    "SELECT *, ST_GeomFromWKB(geom_wkb) AS geom FROM all_points_pdf\n",
    "    WHERE ST_Distance_Spheroid(ST_POINT({lng},{lat}), geom) < 100_000\n",
    "\"\"\").pl()\n",
    "point_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country = \"NLD\"\n",
    "\n",
    "point_geom_refs = point_geoms[GEOMETRY_REF].to_list()\n",
    "point_biz_data = (ct.read_source_emissions(\n",
    "    gas=CO2,\n",
    "    year=2023\n",
    ")\n",
    " .group_by(c_source_id)\n",
    " .agg(c_geometry_ref.first(), c_emissions_quantity.sum(), c_sector.first(), c_subsector.first(), c_source_name.first(), c_lat.first(), c_lon.first())\n",
    " .filter(c_geometry_ref.is_in(point_geom_refs))\n",
    " .filter(c_emissions_quantity != 0)\n",
    " .collect()\n",
    ")\n",
    "point_biz_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "point_geoms_df = point_biz_data.to_pandas()\n",
    "point_geoms_pdf = geopandas.GeoDataFrame(point_geoms_df,\n",
    "                                         geometry=geopandas.points_from_xy(point_geoms_df['lon'],point_geoms_df['lat']), crs=\"EPSG:4326\")\n",
    "point_geoms_pdf.explore(\n",
    "    column=\"sector\",\n",
    "    style_kwds={\"style_function\":lambda x: {\"radius\":np.log(np.abs(x[\"properties\"][\"emissions_quantity\"]))**3/100}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_year = pl.col(\"year\")\n",
    "gadm_biz_data = (ct.read_source_emissions(\n",
    "    gas=CO2,\n",
    "    year=[2022,2023,2024]\n",
    ").with_columns(c_start_time.dt.year().alias(\"year\"))\n",
    " .group_by(c_source_id, c_gas, c_year)\n",
    " .agg(c_geometry_ref.first(), c_emissions_quantity.sum(), c_sector.first(), c_subsector.first(), c_source_name.first(), c_lat.first(), c_lon.first())\n",
    " .filter(c_geometry_ref.is_in(gadms_ref))\n",
    " .filter(c_emissions_quantity != 0)\n",
    " .collect()\n",
    ")\n",
    "gadm_biz_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io\n",
    "plotly.io.templates.default = \"plotly_white\"\n",
    "import plotly.express as px\n",
    "\n",
    "px.bar(\n",
    "gadm_biz_data.filter(c_year == 2024).sort(by=c_emissions_quantity),\n",
    "    x=\"subsector\",\n",
    "    y=\"emissions_quantity\",\n",
    "    color=\"sector\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating optimized parquet files for source data\n",
    "\n",
    "This first section creates files that are the most effective for reading and querying. The general approach is as follows:\n",
    "\n",
    "1. Join the source and source confidence CSV files and writes them as parquet files for each subsector\n",
    "2. Aggregate by year into a yearly parquet file\n",
    "3. Optimize this parquet file for reading\n",
    "\n",
    "This first command creates parquet files that join the source and source confidences for each subsector, and returns a list of all the created files.\n",
    "\n",
    "In this notebook, another trick is to define the transformations as _data functions_. In short, this code will only run if the source code changes. This makes rerunning the notebooks very fast, and only updating when something has changed in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@data_function(\"/data_sources\")\n",
    "def load_sources():\n",
    "    (_, files) = ct.data.load_source_compact()\n",
    "    return files\n",
    "\n",
    "load_sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with the loading, the data is partitioned by year. This is the most relevant for most users: most people are expected to look at specific years and sectors (especially the latest year). This reduces the amount of data to load.\n",
    "\n",
    "Let us have a quick peek at the data in one of these files. It looks already pretty good: a lot of the redundant data such as the enumerations has been deduplicated. All the enumeration data is now converted to integers, this is what `dictionary<values=string, indices=int32, ordered=0>` means. It is not quite ready for high performance however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.parquet import read_table\n",
    "fname = load_sources()[0]\n",
    "print(fname)\n",
    "read_table(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by year and optimizing the output\n",
    "\n",
    "The following block takes all the sector files and aggregates them by year. This is based on the expectation that most users will work on the latest year, and that some users will want to look into the trends across the years.\n",
    "\n",
    "Since these files will be read many times (every time we want to do a graph), it pays off to optimize them. The Parquet format is designed for fast reads of the relevant data. We will do two main optimizations: optimal compression, optimizing the row groups and adding statistics.\n",
    "\n",
    "\n",
    "\n",
    "_Compression_ Parquet allows some data to be compressed by columns. The first intuition is that, looking at each column of data separately, there will be more patterns and thus more opportunities to compress the data. The second intuition is that, in data-intensive application, reading the data is the bottleneck. It is then faster to read smaller compressed data in memory and then decompress it (losing a bit of time in compute), rather than reading larger, uncompressed data. Modern compression algorithms such as ZStandard or LZ4 are designed to be very effective at using a processor. Using them is essentially a pure gain in terms of processing speed.\n",
    "\n",
    "\n",
    "```{admonition} CTODO\n",
    "The year of a data record is defined by its start time. This may be different than the convention used by Climate Trace. To check.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_directory = \"/tmp\"\n",
    "years = ct.data.years\n",
    "version = ct.data.version\n",
    "gases = ct.constants.GAS_LIST\n",
    "\n",
    "@data_function(\"/write_data\")\n",
    "def write_data():\n",
    "    data_files = load_sources()\n",
    "    dfs = []\n",
    "    for tmp_name in data_files:\n",
    "        print(tmp_name)\n",
    "        df = pl.scan_parquet(tmp_name)\n",
    "        df = df.pipe(ct.data.recast_parquet, conf=True)\n",
    "        dfs.append(df)\n",
    "    ldf = pl.concat(dfs)\n",
    "    fnames = []\n",
    "    for gas in gases:\n",
    "        for year in years:\n",
    "            fname1 = f\"{write_directory}/pre_climate_trace-sources_{version}_{year}_{gas}.parquet\"\n",
    "            (\n",
    "                ldf.filter(c_start_time.dt.year() == int(year))\n",
    "                   .filter(c_gas == gas)\n",
    "                   .sort(by=[GAS, SECTOR, SUBSECTOR, ISO3_COUNTRY, SOURCE_ID])\n",
    "                   .sink_parquet(\n",
    "                    fname1,\n",
    "                    compression=\"zstd\",\n",
    "                    maintain_order=True,\n",
    "                    statistics=True,\n",
    "                )\n",
    "            )\n",
    "            fname = f\"{write_directory}/climate_trace-sources_{version}_{year}_{gas}.parquet\"\n",
    "            print(fname)\n",
    "            ds = pyarrow.dataset.dataset(fname1)\n",
    "            pyarrow.dataset.write_dataset(\n",
    "                ds,\n",
    "                base_dir=\"/tmp\",\n",
    "                basename_template=\"ds_{i}.parquet\",\n",
    "                format=\"parquet\",\n",
    "                partitioning=None,\n",
    "                min_rows_per_group=300_000,\n",
    "                max_rows_per_group=1_000_000,\n",
    "            )\n",
    "            shutil.copyfile(\"/tmp/ds_0.parquet\", fname)\n",
    "            fnames.append((fname1, fname))\n",
    "    return fnames\n",
    "\n",
    "write_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optimizing row groups_ A parquet file is a collection of groups of rows, and these rows are organized column-wise along with some statistics. We can choose how many groups to create: the minimum is one group (all the data into a single group), which is the most standard. This is not optimal however: reading can only be done by one processor core at a time. If we have more, they will sit idle. This is why it is better to choose the number of groups to be close to the expected number of processor cores (10-100). When reading, each core will process a different chunk of the file in parallel.\n",
    "\n",
    "Polars cannot do this yet, so the code below directly calls the `pyarrow` package to restructure the final file, calling the function `pyarrow.dataset.write_dataset`. \n",
    "\n",
    "Here is the parquet files produced directly by Polars. It is the result of joining datasets which themselves are the result of reading many files (each by subsector). It is very fragmented (see the `num_row_groups` statistics below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fname_pre, fname_post) = write_data()[0]\n",
    "print(fname_pre)\n",
    "print(fname_post)\n",
    "parquet_file = pyarrow.parquet.ParquetFile(fname_pre)\n",
    "# print(parquet_file.metadata.row_group(0).column(2).statistics)\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final file is more compact: only 58 row groups. It will be much faster to read (up to 50 times faster on my computer) because the readers do not need to gather information from each of the row groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pyarrow.parquet.ParquetFile(fname_post)\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Statistics_ Each row group in a parquet file has statistics. These statistics contain for each columns basic information such as minimum, maximum, etc. as you can see below. During a query, a data system first reads these statistics to check what blocks of data it should read. \n",
    "\n",
    "For example, the first row group only contains agriculture data (which you can infer from `min: agriculture` and `max: agriculture`). As the result, if a query is looking for waste data, it can safely skip this full block. \n",
    "\n",
    "Grouping the rows and creating statistics can dramatically reduce the amount of data being read and processed. Finding the right number of groups is a tradeoff between using more cores to read the data in parallel, and not having to read too many statistics descriptions. In the extreme case of the file created by Polars (5000 row groups), the statistics make up 40% of the file and can take up to 90% of the processing time! If your parquet file reads slowly, it is probably due to its internal layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pyarrow.parquet.ParquetFile(fname_post)\n",
    "parquet_file.metadata.row_group(0).column(12).statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know check that it works correctly. Let's load the newly created data instead of the default version stored on the internet, for the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ct.read_source_emissions(gas=CO2, year=2023, p=\"/tmp\")\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 15M records for this year. This is spread across multiple gas and also multiple trips in the case of boats or airplanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(pl.len()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of distinct source IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec = (sdf\n",
    ".group_by(SOURCE_ID, SECTOR)\n",
    ".agg(pl.len())\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of sources outside forestry and land use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec.filter(c_sector != FORESTRY_AND_LAND_USE).select(pl.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: no source is associated with multiple sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec.group_by(SOURCE_ID).agg(c_sector.n_unique()).filter(pl.col(SECTOR) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: no annual source should be duplicated by gas. It used to be the case with V2 release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf\n",
    ".filter(c_temporal_granularity ==\"annual\")\n",
    ".group_by(SOURCE_ID, GAS)\n",
    ".agg(pl.len())\n",
    ".filter(pl.col(\"len\") > 1)\n",
    ".sort(by=\"len\")\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: emissions should always be defined. V2 used to have empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ct.read_source_emissions(CO2E_100YR, 2023, \"/tmp\")\n",
    "(sdf\n",
    " .select(c_emissions_quantity.is_null().alias(\"null_emissions\"), c_subsector, c_iso3_country)\n",
    " .group_by(c_subsector, \"null_emissions\")\n",
    " .agg(pl.len())\n",
    " .collect()\n",
    " .pivot(index=SUBSECTOR, on=\"null_emissions\", values=\"len\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrity checks\n",
    "\n",
    "Before uploading and publishing data, it is a good idea to run a number of checks. Frameworks such as [pandera](https://pandera.readthedocs.io/en/latest/polars.html) are very helpful to implement these checks. Here we just check that Akrotiri and Dhekelia (country code XAD) is not included, as mentioned in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ct.read_source_emissions(gas=GAS_LIST, year=2022, p=\"/tmp\")\n",
    " .filter(c_iso3_country == \"XAD\")\n",
    " .select(pl.len())\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CO2e subsector data should be a superset of all sectors\n",
    "\n",
    "Here is an example of issue to investigate: one would expect the total CO2e_100yr (total emissions normalized by their CO2 equivalent) to be at least present for each sector in which emissions are reported. This is not the case for FLU, for instance for the `removals` subsector.\n",
    "\n",
    "```{admonition} CTODO\n",
    ":name: missing-co2e-subsectors\n",
    "Confirm with CT.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=20):\n",
    "    print(ct.read_source_emissions(gas=GAS_LIST, year=2022, p=\"/tmp\")\n",
    "     .group_by(c_sector, c_subsector, c_gas)\n",
    "     .agg(c_emissions_quantity.sum())\n",
    "     #.filter(c_emissions_quantity < 0)\n",
    "     .sort(by=[c_sector, c_subsector, c_gas])\n",
    "     .collect()\n",
    "     .pivot(GAS, index=[SECTOR, SUBSECTOR])\n",
    "     .filter(pl.col(CO2E_100YR).is_null())\n",
    "     .filter(pl.col(CO2) != 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parquet files for country emissions\n",
    "\n",
    "As of V3, country emission data is also large enough that it should be compacted in parquet files. Note the dramatic difference:\n",
    "\n",
    "- uncompressed CSV file: 106MB\n",
    "- compressed CSV file: 6MB\n",
    "- parquet: 1MB !!\n",
    "\n",
    "As highlighted, the parquet file also has the advantage of being very efficient at extracting only the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the official archives, read all the gases.\n",
    "\n",
    "@data_function(\"/read_country\")\n",
    "def read_country():\n",
    "    path = Path(tempfile.gettempdir()) / f\"climate-trace-countries-{ct.data.version}.parquet\"\n",
    "    print(path)\n",
    "    cdf = ct.read_country_emissions(ct.constants.GAS_LIST, archive_path=True)\n",
    "    # Optimizing to read by time and then gas.\n",
    "    # The logic being that country-specific files are already available from CT.\n",
    "    (cdf\n",
    "     .sort(by=[c_start_time,c_gas,c_iso3_country])\n",
    "      .write_parquet(path) # Not taking precautions, the file is so small.\n",
    "    )\n",
    "    return path\n",
    "\n",
    "p = read_country()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contry emissions: integrity checks\n",
    "\n",
    "In a production pipeline, before uploading the final data, we would run a number of checks again on the country emissions. Here are a few checks that we can run (and which are currently failing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = ct.read_country_emissions(parquet_path=p)\n",
    "cdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country emissions: CO2e data should be a superset of all country emissions\n",
    "\n",
    "We see that some subsectors are present in CO2 emissions but are missing in the aggregated CO2e emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=20):\n",
    "    print(cdf\n",
    "     .group_by(c_sector, c_subsector, c_gas)\n",
    "     .agg(c_emissions_quantity.sum())\n",
    "     .sort(by=[c_sector, c_subsector, c_gas])\n",
    "     .pivot(GAS, index=[SECTOR, SUBSECTOR])\n",
    "     .filter(pl.col(CO2E_100YR).is_null())\n",
    "     .filter(pl.col(CO2) != 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country emissions: some countries are excluded from the dataset\n",
    "\n",
    "The Climate TRACE documentation excludes certain countries from the final release, but they are still present in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_isos = [\"XAD\", \"XCL\", \"XPI\", \"XSP\"]\n",
    "(cdf\n",
    " .filter(c_iso3_country.is_in(excluded_isos))\n",
    " .group_by([ISO3_COUNTRY, c_start_time.dt.year(), GAS, SECTOR, SUBSECTOR])\n",
    " .agg(pl.len()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the Hugging Face Hub\n",
    "\n",
    "As a final step, we make the datasets available on Hugging Face as a downloadable dataset.\n",
    "\n",
    "This step will only work if you have the credentials to upload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub.utils\n",
    "upload = False\n",
    "if upload:\n",
    "    try:\n",
    "        api = huggingface_hub.HfApi()\n",
    "        for (_, fpath) in write_data():\n",
    "            fname = os.path.basename(fpath)\n",
    "            print(fname, fpath)\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=fpath,\n",
    "                path_in_repo=fname,\n",
    "                repo_id=\"tjhunter/climate-trace\",\n",
    "                repo_type=\"dataset\",\n",
    "            )\n",
    "        fpath = read_country()\n",
    "        fname = os.path.basename(fpath)\n",
    "        print(fname, fpath)\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=fname,\n",
    "            repo_id=\"tjhunter/climate-trace\",\n",
    "            repo_type=\"dataset\",\n",
    "        )\n",
    "    except huggingface_hub.utils.HfHubHTTPError as e:\n",
    "        print(\"error\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion and formatting\n",
    "\n",
    "This notebook explains how to convert the Climate TRACE dataset to a format that is more appropriate for data science. \n",
    "\n",
    "```{note}\n",
    "This section is relevant for data engineers, or data scientists who want to understand how the data \n",
    "has been prepared. Skip if you just want to access the final, prepared data.\n",
    "```\n",
    "\n",
    "The original data from Climate TRACE is offered as a series of CSV files bundled in ZIP archives. That format is universally understood, but it is not the most effective for effective analysis with data science tools. In particular, it is large: the source data, uncompressed, is about 20GB. This is the size at which most people would consider this project to be \"big data\" or at least \"medium data\". With the proper choice of data storage, we will bring it down to a breezy \"small data\" without losing information along the way.\n",
    "\n",
    "Instead, we are going to use the Parquet format. This format has a number of advantages:\n",
    "- it is _column-based_ : data systems can process big chunks of data at once, rather than line by line. Also, depending on the information requested, systems will read only the relevant columns and skip the rest very effectively\n",
    "- it is _universal_ : most modern data systems will be able to read it\n",
    "- it is _structured_ : basic information about numbers, categories, ... are preserved. It \n",
    "\n",
    "\n",
    "Looking at the code, we are performing a few tricks:\n",
    "\n",
    "_Compacting the data_ We minimize the size of the files by taking advantage of its structures. In particular, we know in many cases that values are part of known enumerations (sectors, ...). We replace all these by `polars.Enumeration`s. Not only this makes files smaller, but it also allows data systems to make clever optimization for complex operations such as joining.\n",
    "\n",
    "_Lazy reading_ If we were to read all the source data using a traditional system such as Excel or Pandas, we would require a serious amount of memory. The files themselves are more than 5GB. Polars is capable of reading straight from the zip file in a streaming fashion. This is what Polars calls a Lazy dataframe, or LazyFrame. Even when doing complicated operations such as joining the source files with the confidence information, Polars only uses 3GB of memory on my machine. In fact, this way of working is so fast that the `ctrace` package directly reads all the country emissions data from the zip files in less than a second.\n",
    "\n",
    "_Using known enumerations_ You will see in the source code that nearly all the variables such as column names, names of gas and sectors, etc. are replaced CONSTANT_NAMES such as `CH4`,.... You can use that to autocomplete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from ctrace.constants import *\n",
    "import ctrace as ct\n",
    "import pyarrow\n",
    "from dds import data_function\n",
    "import shutil\n",
    "import dds\n",
    "import huggingface_hub\n",
    "logging.getLogger(\"dds\").setLevel(logging.WARNING)\n",
    "dds.accept_module(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating optimized parquet files\n",
    "\n",
    "This first section creates files that are the most effective for reading and querying. The general approach is as follows:\n",
    "\n",
    "1. Join the source and source confidence CSV files and writes them as parquet files for each subsector\n",
    "2. Aggregate by year into a yearly parquet file\n",
    "3. Optimize this parquet file for reading\n",
    "\n",
    "This first command creates parquet files that join the source and source confidences for each subsector, and returns a list of all the created files.\n",
    "\n",
    "In this notebook, another trick is to define the transformations as _data functions_. In short, this code will only run if the source code changes. This makes rerunning the notebooks very fast, and only updating when something has changed in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/tmp/cropland-fires_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/enteric-fermentation-cattle-pasture_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/manure-left-on-pasture-cattle_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/rice-cultivation_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/synthetic-fertilizer-application_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/net-forest-land_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/net-shrubgrass_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/net-wetland_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/water-reservoirs_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/coal-mining_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/oil-and-gas-refining_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/aluminum_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/cement_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/chemicals_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/other-manufacturing_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/pulp-and-paper_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/bauxite-mining_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/copper-mining_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/iron-mining_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/electricity-generation_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/domestic-aviation_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/domestic-shipping_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/international-aviation_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/international-shipping_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/road-transportation_emissions_sources.parquet'),\n",
       " PosixPath('/tmp/solid-waste-disposal_emissions_sources.parquet')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@data_function(\"/data_sources\")\n",
    "def load_sources():\n",
    "    (_, files) = ct.data.load_source_compact()\n",
    "    return files\n",
    "\n",
    "load_sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is loaded lazily, this step takes only 300MB of memory on my machine. Not bad for producing 2GB of data!\n",
    "\n",
    "To help with the loading, the data is partitioned by year. This is the most relevant for most users: most people are expected to look at specific years and sectors (especially the latest year). This reduces the amount of data to load.\n",
    "\n",
    "Let us have a quick peek at the data in one of these files. It looks already pretty good: a lot of the redundant data such as the enumerations has been deduplicated. All the enumeration data is now converted to integers, this is what `dictionary<values=string, indices=int32, ordered=0>` means. It is not quite ready for high performance however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/cropland-fires_emissions_sources.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "source_id: uint64\n",
       "iso3_country: dictionary<values=string, indices=int32, ordered=0>\n",
       "sector: dictionary<values=string, indices=int32, ordered=0>\n",
       "subsector: dictionary<values=string, indices=int32, ordered=0>\n",
       "original_inventory_sector: dictionary<values=string, indices=int32, ordered=0>\n",
       "start_time: timestamp[ms, tz=UTC]\n",
       "end_time: timestamp[ms, tz=UTC]\n",
       "temporal_granularity: dictionary<values=string, indices=int32, ordered=0>\n",
       "gas: dictionary<values=string, indices=int32, ordered=0>\n",
       "emissions_quantity: double\n",
       "emissions_factor: double\n",
       "emissions_factor_units: large_string\n",
       "capacity: double\n",
       "capacity_units: large_string\n",
       "capacity_factor: double\n",
       "activity: double\n",
       "activity_units: large_string\n",
       "created_date: timestamp[ms, tz=UTC]\n",
       "modified_date: timestamp[ms, tz=UTC]\n",
       "source_name: large_string\n",
       "source_type: large_string\n",
       "lat: double\n",
       "lon: double\n",
       "other1: large_string\n",
       "other2: large_string\n",
       "other3: large_string\n",
       "other4: large_string\n",
       "other5: large_string\n",
       "other6: large_string\n",
       "other7: large_string\n",
       "other8: large_string\n",
       "other9: large_string\n",
       "other10: large_string\n",
       "other11: large_string\n",
       "other12: large_string\n",
       "other1_def: large_string\n",
       "other2_def: large_string\n",
       "other3_def: large_string\n",
       "other4_def: large_string\n",
       "other5_def: large_string\n",
       "other6_def: large_string\n",
       "other7_def: large_string\n",
       "other8_def: large_string\n",
       "other9_def: large_string\n",
       "other10_def: large_string\n",
       "other11_def: large_string\n",
       "other12_def: large_string\n",
       "geometry_ref: large_string\n",
       "conf_source_type: dictionary<values=string, indices=int32, ordered=0>\n",
       "conf_capacity: dictionary<values=string, indices=int32, ordered=0>\n",
       "conf_capacity_factor: dictionary<values=string, indices=int32, ordered=0>\n",
       "conf_activity: dictionary<values=string, indices=int32, ordered=0>\n",
       "conf_emissions_factor: dictionary<values=string, indices=int32, ordered=0>\n",
       "conf_emissions_quantity: dictionary<values=string, indices=int32, ordered=0>\n",
       "----\n",
       "source_id: [[10760226,10760226,10760226,10760226,10760226,...,10792237,10792237,10792237,10792237,10792237],[10792237,10792237,10792237,10792237,10792237,...,10888525,10888525,10888525,10888525,10888525],...,[11299477,11299477,11299477,11299477,11299477,...,11301601,11301601,11301601,11301601,11301601],[11301601,11301601,11301601,11301601,11301601,...,11303229,11303229,11303229,11303229,11303229]]\n",
       "iso3_country: [  -- dictionary:\n",
       "[\"ABW\",\"AFG\",\"AGO\",\"AIA\",\"ALA\",...,\"ZWE\",\"ZNC\",\"UNK\",\"SCG\",\"XAD\"]  -- indices:\n",
       "[1,1,1,1,1,...,23,23,23,23,23],  -- dictionary:\n",
       "[\"ABW\",\"AFG\",\"AGO\",\"AIA\",\"ALA\",...,\"ZWE\",\"ZNC\",\"UNK\",\"SCG\",\"XAD\"]  -- indices:\n",
       "[23,23,23,23,23,...,32,32,32,32,32],...,  -- dictionary:\n",
       "[\"ABW\",\"AFG\",\"AGO\",\"AIA\",\"ALA\",...,\"ZWE\",\"ZNC\",\"UNK\",\"SCG\",\"XAD\"]  -- indices:\n",
       "[234,234,234,234,234,...,238,238,238,238,238],  -- dictionary:\n",
       "[\"ABW\",\"AFG\",\"AGO\",\"AIA\",\"ALA\",...,\"ZWE\",\"ZNC\",\"UNK\",\"SCG\",\"XAD\"]  -- indices:\n",
       "[238,238,238,238,238,...,249,249,249,249,249]]\n",
       "sector: [  -- dictionary:\n",
       "[\"agriculture\",\"buildings\",\"fluorinated-gases\",\"forestry-and-land-use\",\"fossil-fuel-operations\",\"manufacturing\",\"mineral-extraction\",\"power\",\"transportation\",\"waste\"]  -- indices:\n",
       "[0,0,0,0,0,...,0,0,0,0,0],  -- dictionary:\n",
       "[\"agriculture\",\"buildings\",\"fluorinated-gases\",\"forestry-and-land-use\",\"fossil-fuel-operations\",\"manufacturing\",\"mineral-extraction\",\"power\",\"transportation\",\"waste\"]  -- indices:\n",
       "[0,0,0,0,0,...,0,0,0,0,0],...,  -- dictionary:\n",
       "[\"agriculture\",\"buildings\",\"fluorinated-gases\",\"forestry-and-land-use\",\"fossil-fuel-operations\",\"manufacturing\",\"mineral-extraction\",\"power\",\"transportation\",\"waste\"]  -- indices:\n",
       "[0,0,0,0,0,...,0,0,0,0,0],  -- dictionary:\n",
       "[\"agriculture\",\"buildings\",\"fluorinated-gases\",\"forestry-and-land-use\",\"fossil-fuel-operations\",\"manufacturing\",\"mineral-extraction\",\"power\",\"transportation\",\"waste\"]  -- indices:\n",
       "[0,0,0,0,0,...,0,0,0,0,0]]\n",
       "subsector: [  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"textiles-leather-apparel\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\",\"wood-and-wood-products\"]  -- indices:\n",
       "[7,7,7,7,7,...,7,7,7,7,7],  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"textiles-leather-apparel\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\",\"wood-and-wood-products\"]  -- indices:\n",
       "[7,7,7,7,7,...,7,7,7,7,7],...,  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"textiles-leather-apparel\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\",\"wood-and-wood-products\"]  -- indices:\n",
       "[7,7,7,7,7,...,7,7,7,7,7],  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"textiles-leather-apparel\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\",\"wood-and-wood-products\"]  -- indices:\n",
       "[7,7,7,7,7,...,7,7,7,7,7]]\n",
       "original_inventory_sector: [  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"steel\",\"synthetic-fertilizer-application\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\"]  -- indices:\n",
       "[null,null,null,null,null,...,null,null,null,null,null],  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"steel\",\"synthetic-fertilizer-application\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\"]  -- indices:\n",
       "[null,null,null,null,null,...,null,null,null,null,null],...,  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"steel\",\"synthetic-fertilizer-application\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\"]  -- indices:\n",
       "[null,null,null,null,null,...,null,null,null,null,null],  -- dictionary:\n",
       "[\"aluminum\",\"bauxite-mining\",\"biological-treatment-of-solid-waste-and-biogenic\",\"cement\",\"chemicals\",...,\"steel\",\"synthetic-fertilizer-application\",\"wastewater-treatment-and-discharge\",\"water-reservoirs\",\"wetland-fires\"]  -- indices:\n",
       "[null,null,null,null,null,...,null,null,null,null,null]]\n",
       "start_time: [[2021-01-01 00:00:00.000Z,2021-02-01 00:00:00.000Z,2021-03-01 00:00:00.000Z,2021-04-01 00:00:00.000Z,2021-05-01 00:00:00.000Z,...,2023-04-01 00:00:00.000Z,2023-05-01 00:00:00.000Z,2023-06-01 00:00:00.000Z,2023-07-01 00:00:00.000Z,2023-08-01 00:00:00.000Z],[2023-09-01 00:00:00.000Z,2023-10-01 00:00:00.000Z,2023-11-01 00:00:00.000Z,2023-12-01 00:00:00.000Z,2024-01-01 00:00:00.000Z,...,2021-12-01 00:00:00.000Z,2022-01-01 00:00:00.000Z,2022-02-01 00:00:00.000Z,2022-03-01 00:00:00.000Z,2022-04-01 00:00:00.000Z],...,[2023-09-01 00:00:00.000Z,2023-10-01 00:00:00.000Z,2023-11-01 00:00:00.000Z,2023-12-01 00:00:00.000Z,2024-01-01 00:00:00.000Z,...,2021-12-01 00:00:00.000Z,2022-01-01 00:00:00.000Z,2022-02-01 00:00:00.000Z,2022-03-01 00:00:00.000Z,2022-04-01 00:00:00.000Z],[2022-05-01 00:00:00.000Z,2022-06-01 00:00:00.000Z,2022-07-01 00:00:00.000Z,2022-08-01 00:00:00.000Z,2022-09-01 00:00:00.000Z,...,2024-08-01 00:00:00.000Z,2024-09-01 00:00:00.000Z,2024-10-01 00:00:00.000Z,2024-11-01 00:00:00.000Z,2024-12-01 00:00:00.000Z]]\n",
       "end_time: [[2021-01-31 00:00:00.000Z,2021-02-28 00:00:00.000Z,2021-03-31 00:00:00.000Z,2021-04-30 00:00:00.000Z,2021-05-31 00:00:00.000Z,...,2023-04-30 00:00:00.000Z,2023-05-31 00:00:00.000Z,2023-06-30 00:00:00.000Z,2023-07-31 00:00:00.000Z,2023-08-31 00:00:00.000Z],[2023-09-30 00:00:00.000Z,2023-10-31 00:00:00.000Z,2023-11-30 00:00:00.000Z,2023-12-31 00:00:00.000Z,2024-01-31 00:00:00.000Z,...,2021-12-31 00:00:00.000Z,2022-01-31 00:00:00.000Z,2022-02-28 00:00:00.000Z,2022-03-31 00:00:00.000Z,2022-04-30 00:00:00.000Z],...,[2023-09-30 00:00:00.000Z,2023-10-31 00:00:00.000Z,2023-11-30 00:00:00.000Z,2023-12-31 00:00:00.000Z,2024-01-31 00:00:00.000Z,...,2021-12-31 00:00:00.000Z,2022-01-31 00:00:00.000Z,2022-02-28 00:00:00.000Z,2022-03-31 00:00:00.000Z,2022-04-30 00:00:00.000Z],[2022-05-31 00:00:00.000Z,2022-06-30 00:00:00.000Z,2022-07-31 00:00:00.000Z,2022-08-31 00:00:00.000Z,2022-09-30 00:00:00.000Z,...,2024-08-31 00:00:00.000Z,2024-09-30 00:00:00.000Z,2024-10-31 00:00:00.000Z,2024-11-30 00:00:00.000Z,2024-12-31 00:00:00.000Z]]\n",
       "temporal_granularity: [  -- dictionary:\n",
       "[\"annual\",\"other\",\"month\",\"week\",\"day\",\"hour\"]  -- indices:\n",
       "[2,2,2,2,2,...,2,2,2,2,2],  -- dictionary:\n",
       "[\"annual\",\"other\",\"month\",\"week\",\"day\",\"hour\"]  -- indices:\n",
       "[2,2,2,2,2,...,2,2,2,2,2],...,  -- dictionary:\n",
       "[\"annual\",\"other\",\"month\",\"week\",\"day\",\"hour\"]  -- indices:\n",
       "[2,2,2,2,2,...,2,2,2,2,2],  -- dictionary:\n",
       "[\"annual\",\"other\",\"month\",\"week\",\"day\",\"hour\"]  -- indices:\n",
       "[2,2,2,2,2,...,2,2,2,2,2]]\n",
       "gas: [  -- dictionary:\n",
       "[\"co2\",\"ch4\",\"n2o\",\"co2e_100yr\",\"co2e_20yr\"]  -- indices:\n",
       "[3,3,3,3,3,...,3,3,3,3,3],  -- dictionary:\n",
       "[\"co2\",\"ch4\",\"n2o\",\"co2e_100yr\",\"co2e_20yr\"]  -- indices:\n",
       "[3,3,3,3,3,...,3,3,3,3,3],...,  -- dictionary:\n",
       "[\"co2\",\"ch4\",\"n2o\",\"co2e_100yr\",\"co2e_20yr\"]  -- indices:\n",
       "[3,3,3,3,3,...,3,3,3,3,3],  -- dictionary:\n",
       "[\"co2\",\"ch4\",\"n2o\",\"co2e_100yr\",\"co2e_20yr\"]  -- indices:\n",
       "[3,3,3,3,3,...,3,3,3,3,3]]\n",
       "emissions_quantity: [[5.12882723971024,9.598594377581522,48.20951841731986,65.14562967577841,40.547458405758434,...,278.8629388807241,181.72725125031533,94.74449436621036,60.87353845316802,83.76946388430098],[219.0641185300356,302.7970750819214,201.66674777319383,87.92596602353214,45.28234324389594,...,29.78787247912232,17.335694807550862,24.58390316188942,71.07676857520285,97.93964561397163],...,[0.2345260927873606,0.3241690853021531,0.2159007815508435,0.0941319528018729,0.0484784596606174,...,8.139068550822351,4.45458886955816,6.317092139136458,18.26392225371616,25.166620668151072],[16.53790662657551,8.06833139365578,4.805707224347403,6.715882823799204,18.58601872257653,...,24.49115051809122,69.2276843433509,95.45684096459736,62.921474528536265,29.98144256385381]]\n",
       "..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.parquet import read_table\n",
    "fname = load_sources()[0]\n",
    "print(fname)\n",
    "read_table(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by year and optimizing the output\n",
    "\n",
    "The following block takes all the sector files and aggregates them by year. This is based on the expectation that most users will work on the latest year, and that some users will want to look into the trends across the years.\n",
    "\n",
    "Since these files will be read many times (every time we want to do a graph), it pays off to optimize them. The Parquet format is designed for fast reads of the relevant data. We will do two main optimizations: optimal compression, optimizing the row groups and adding statistics.\n",
    "\n",
    "\n",
    "\n",
    "_Compression_ Parquet allows some data to be compressed by columns. The first intuition is that, looking at each column of data separately, there will be more patterns and thus more opportunities to compress the data. The second intuition is that, in data-intensive application, reading the data is the bottleneck. It is then faster to read smaller compressed data in memory and then decompress it (losing a bit of time in compute), rather than reading larger, uncompressed data. Modern compression algorithms such as ZStandard or LZ4 are designed to be very effective at using a processor. Using them is essentially a pure gain in terms of processing speed.\n",
    "\n",
    "\n",
    "```{admonition} CTODO\n",
    "The year of a data record is defined by its start time. This may be different than the convention used by Climate Trace. To check.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/climate_trace-sources_v3-2024-ct3_2021.parquet\n",
      "/tmp/climate_trace-sources_v3-2024-ct3_2022.parquet\n",
      "/tmp/climate_trace-sources_v3-2024-ct3_2023.parquet\n",
      "/tmp/climate_trace-sources_v3-2024-ct3_2024.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/tmp/pre_climate_trace-sources_v3-2024-ct3_2021.parquet',\n",
       "  '/tmp/climate_trace-sources_v3-2024-ct3_2021.parquet'),\n",
       " ('/tmp/pre_climate_trace-sources_v3-2024-ct3_2022.parquet',\n",
       "  '/tmp/climate_trace-sources_v3-2024-ct3_2022.parquet'),\n",
       " ('/tmp/pre_climate_trace-sources_v3-2024-ct3_2023.parquet',\n",
       "  '/tmp/climate_trace-sources_v3-2024-ct3_2023.parquet'),\n",
       " ('/tmp/pre_climate_trace-sources_v3-2024-ct3_2024.parquet',\n",
       "  '/tmp/climate_trace-sources_v3-2024-ct3_2024.parquet')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_directory = \"/tmp\"\n",
    "years = ct.data.years\n",
    "version = ct.data.version\n",
    "\n",
    "@data_function(\"/write_data\")\n",
    "def write_data():\n",
    "    data_files = load_sources()\n",
    "    dfs = []\n",
    "    for tmp_name in data_files:\n",
    "        df = pl.scan_parquet(tmp_name)\n",
    "        df = df.pipe(ct.data.recast_parquet, conf=True)\n",
    "        dfs.append(df)\n",
    "    ldf = pl.concat(dfs)\n",
    "    fnames = []\n",
    "    for year in years:\n",
    "        fname1 = f\"{write_directory}/pre_climate_trace-sources_{version}_{year}.parquet\"\n",
    "        (\n",
    "            ldf.filter(c_start_time.dt.year() == int(year))\n",
    "               .sort(by=[GAS, SECTOR, SUBSECTOR, ISO3_COUNTRY, SOURCE_ID])\n",
    "               .sink_parquet(\n",
    "                fname1,\n",
    "                compression=\"zstd\",\n",
    "                maintain_order=True,\n",
    "                statistics=True,\n",
    "            )\n",
    "        )\n",
    "        fname = f\"{write_directory}/climate_trace-sources_{version}_{year}.parquet\"\n",
    "        print(fname)\n",
    "        ds = pyarrow.dataset.dataset(fname1)\n",
    "        pyarrow.dataset.write_dataset(\n",
    "            ds,\n",
    "            base_dir=\"/tmp\",\n",
    "            basename_template=\"ds_{i}.parquet\",\n",
    "            format=\"parquet\",\n",
    "            partitioning=None,\n",
    "            min_rows_per_group=300_000,\n",
    "            max_rows_per_group=1_000_000,\n",
    "        )\n",
    "        shutil.copyfile(\"/tmp/ds_0.parquet\", fname)\n",
    "        fnames.append((fname1, fname))\n",
    "    return fnames\n",
    "\n",
    "write_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optimizing row groups_ A parquet file is a collection of groups of rows, and these rows are organized column-wise along with some statistics. We can choose how many groups to create: the minimum is one group (all the data into a single group), which is the most standard. This is not optimal however: reading can only be done by one processor core at a time. If we have more, they will sit idle. This is why it is better to choose the number of groups to be close to the expected number of processor cores (10-100). When reading, each core will process a different chunk of the file in parallel.\n",
    "\n",
    "Polars cannot do this yet, so the code below directly calls the `pyarrow` package to restructure the final file, calling the function `pyarrow.dataset.write_dataset`. \n",
    "\n",
    "Here is the parquet files produced directly by Polars. It is the result of joining datasets which themselves are the result of reading many files (each by subsector). It is very fragmented (see the `num_row_groups` statistics below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/pre_climate_trace-sources_v3-2024-ct3_2021.parquet\n",
      "/tmp/climate_trace-sources_v3-2024-ct3_2021.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7d8b711750d0>\n",
       "  created_by: Polars\n",
       "  num_columns: 54\n",
       "  num_rows: 6605664\n",
       "  num_row_groups: 8\n",
       "  format_version: 1.0\n",
       "  serialized_size: 46658"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fname_pre, fname_post) = write_data()[0]\n",
    "print(fname_pre)\n",
    "print(fname_post)\n",
    "parquet_file = pyarrow.parquet.ParquetFile(fname_pre)\n",
    "# print(parquet_file.metadata.row_group(0).column(2).statistics)\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final file is more compact: only 58 row groups. It will be much faster to read (up to 50 times faster on my computer) because the readers do not need to gather information from each of the row groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7d8b4ebdc950>\n",
       "  created_by: parquet-cpp-arrow version 15.0.2\n",
       "  num_columns: 54\n",
       "  num_rows: 6605664\n",
       "  num_row_groups: 21\n",
       "  format_version: 2.6\n",
       "  serialized_size: 117610"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = pyarrow.parquet.ParquetFile(fname_post)\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Statistics_ Each row group in a parquet file has statistics. These statistics contain for each columns basic information such as minimum, maximum, etc. as you can see below. During a query, a data system first reads these statistics to check what blocks of data it should read. \n",
    "\n",
    "For example, the first row group only contains agriculture data (which you can infer from `min: agriculture` and `max: agriculture`). As the result, if a query is looking for waste data, it can safely skip this full block. \n",
    "\n",
    "Grouping the rows and creating statistics can dramatically reduce the amount of data being read and processed. Finding the right number of groups is a tradeoff between using more cores to read the data in parallel, and not having to read too many statistics descriptions. In the extreme case of the file created by Polars (5000 row groups), the statistics make up 40% of the file and can take up to 90% of the processing time! If your parquet file reads slowly, it is probably due to its internal layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.Statistics object at 0x7d8b4ebdce50>\n",
       "  has_min_max: False\n",
       "  min: None\n",
       "  max: None\n",
       "  null_count: 327680\n",
       "  distinct_count: None\n",
       "  num_values: 0\n",
       "  physical_type: BYTE_ARRAY\n",
       "  logical_type: String\n",
       "  converted_type (legacy): UTF8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = pyarrow.parquet.ParquetFile(fname_post)\n",
    "parquet_file.metadata.row_group(0).column(20).statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know check that it works correctly. Let's load the newly created data instead of the default version stored on the internet, for the year 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>NAIVE QUERY PLAN</h4><p>run <b>LazyFrame.show_graph()</b> to see the optimized version</p><?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: polars_query Pages: 1 -->\n",
       "<svg width=\"3898pt\" height=\"190pt\"\n",
       " viewBox=\"0.00 0.00 3898.00 190.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 186)\">\n",
       "<title>polars_query</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-186 3894,-186 3894,4 -4,4\"/>\n",
       "<!-- p1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>p1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"3890,-182 0,-182 0,-146 3890,-146 3890,-182\"/>\n",
       "<text text-anchor=\"middle\" x=\"1945\" y=\"-160.3\" font-family=\"Times,serif\" font-size=\"14.00\">WITH COLUMNS [col(&quot;conf_source_type&quot;).cast(Enum(Some(local), Physical)).alias(&quot;conf_source_type&quot;), col(&quot;conf_capacity&quot;).cast(Enum(Some(local), Physical)).alias(&quot;conf_capacity&quot;), col(&quot;conf_capacity_factor&quot;).cast(Enum(Some(local), Physical)).alias(&quot;conf_capacity_factor&quot;), col(&quot;conf_activity&quot;).cast(Enum(Some(local), Physical)).alias(&quot;conf_activity&quot;), col(&quot;conf_emissions_factor&quot;).cast(Enum(Some(local), Physical)).alias(&quot;conf_emissions_factor&quot;), col(&quot;conf_emissions_quantity&quot;).cast(Enum(Some(local), Physical)).alias(&quot;conf_emissions_quantity&quot;)]</text>\n",
       "</g>\n",
       "<!-- p2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>p2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"3396.5,-110 493.5,-110 493.5,-74 3396.5,-74 3396.5,-110\"/>\n",
       "<text text-anchor=\"middle\" x=\"1945\" y=\"-88.3\" font-family=\"Times,serif\" font-size=\"14.00\">WITH COLUMNS [col(&quot;iso3_country&quot;).strict_cast(Enum(Some(local), Physical)).alias(&quot;iso3_country&quot;), col(&quot;gas&quot;).cast(Enum(Some(local), Physical)).alias(&quot;gas&quot;), col(&quot;temporal_granularity&quot;).strict_cast(Enum(Some(local), Physical)).alias(&quot;temporal_granularity&quot;), col(&quot;subsector&quot;).strict_cast(Enum(Some(local), Physical)).alias(&quot;subsector&quot;), col(&quot;sector&quot;).strict_cast(Enum(Some(local), Physical)).alias(&quot;sector&quot;)]</text>\n",
       "</g>\n",
       "<!-- p1&#45;&#45;p2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>p1&#45;&#45;p2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1945,-145.7C1945,-134.85 1945,-120.92 1945,-110.1\"/>\n",
       "</g>\n",
       "<!-- p3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>p3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2206,-38 1684,-38 1684,0 2206,0 2206,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"1945\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">Parquet SCAN [/tmp/climate_trace&#45;sources_v3&#45;2024&#45;ct3_2022.parquet]</text>\n",
       "<text text-anchor=\"middle\" x=\"1945\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">π */54;</text>\n",
       "</g>\n",
       "<!-- p2&#45;&#45;p3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>p2&#45;&#45;p3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1945,-73.81C1945,-62.98 1945,-49.01 1945,-38.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<LazyFrame at 0x7D8B4E1354B0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = ct.read_source_emissions(year=2022, p=\"/tmp\")\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 6M records for this year. This is spread across multiple gas and also multiple trips in the case of boats or airplanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>len</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>6605664</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌─────────┐\n",
       "│ len     │\n",
       "│ ---     │\n",
       "│ u32     │\n",
       "╞═════════╡\n",
       "│ 6605664 │\n",
       "└─────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.select(pl.len()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of distinct source IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sec = (sdf\n",
    ".group_by(SOURCE_ID, SECTOR)\n",
    ".agg(pl.len())\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of sources outside FLU:\n",
    "\n",
    "```{admonition} CTODO\n",
    "This number does not match the official number on the Climate Trace website (395075 for 2022). Investigate.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>len</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>374923</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌────────┐\n",
       "│ len    │\n",
       "│ ---    │\n",
       "│ u32    │\n",
       "╞════════╡\n",
       "│ 374923 │\n",
       "└────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_sec.filter(c_sector != FORESTRY_AND_LAND_USE).select(pl.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: no source is associated with multiple sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_id</th><th>sector</th></tr><tr><td>u64</td><td>u32</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 2)\n",
       "┌───────────┬────────┐\n",
       "│ source_id ┆ sector │\n",
       "│ ---       ┆ ---    │\n",
       "│ u64       ┆ u32    │\n",
       "╞═══════════╪════════╡\n",
       "└───────────┴────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_sec.group_by(SOURCE_ID).agg(c_sector.n_unique()).filter(pl.col(SECTOR) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: no annual source should be duplicated by gas. It used to be the case with V2 release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_id</th><th>gas</th><th>len</th></tr><tr><td>u64</td><td>enum</td><td>u32</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 3)\n",
       "┌───────────┬──────┬─────┐\n",
       "│ source_id ┆ gas  ┆ len │\n",
       "│ ---       ┆ ---  ┆ --- │\n",
       "│ u64       ┆ enum ┆ u32 │\n",
       "╞═══════════╪══════╪═════╡\n",
       "└───────────┴──────┴─────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sdf\n",
    ".filter(c_temporal_granularity ==\"annual\")\n",
    ".group_by(SOURCE_ID, GAS)\n",
    ".agg(pl.len())\n",
    ".filter(pl.col(\"len\") > 1)\n",
    ".sort(by=\"len\")\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: emissions should always be defined. V2 used to have empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (26, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subsector</th><th>false</th></tr><tr><td>enum</td><td>u32</td></tr></thead><tbody><tr><td>&quot;road-transportation&quot;</td><td>684168</td></tr><tr><td>&quot;domestic-aviation&quot;</td><td>58920</td></tr><tr><td>&quot;net-forest-land&quot;</td><td>679632</td></tr><tr><td>&quot;iron-mining&quot;</td><td>8664</td></tr><tr><td>&quot;manure-left-on-pasture-cattle&quot;</td><td>607680</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;domestic-shipping&quot;</td><td>99132</td></tr><tr><td>&quot;cement&quot;</td><td>26892</td></tr><tr><td>&quot;net-shrubgrass&quot;</td><td>682260</td></tr><tr><td>&quot;rice-cultivation&quot;</td><td>684408</td></tr><tr><td>&quot;other-manufacturing&quot;</td><td>4776</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (26, 2)\n",
       "┌───────────────────────────────┬────────┐\n",
       "│ subsector                     ┆ false  │\n",
       "│ ---                           ┆ ---    │\n",
       "│ enum                          ┆ u32    │\n",
       "╞═══════════════════════════════╪════════╡\n",
       "│ road-transportation           ┆ 684168 │\n",
       "│ domestic-aviation             ┆ 58920  │\n",
       "│ net-forest-land               ┆ 679632 │\n",
       "│ iron-mining                   ┆ 8664   │\n",
       "│ manure-left-on-pasture-cattle ┆ 607680 │\n",
       "│ …                             ┆ …      │\n",
       "│ domestic-shipping             ┆ 99132  │\n",
       "│ cement                        ┆ 26892  │\n",
       "│ net-shrubgrass                ┆ 682260 │\n",
       "│ rice-cultivation              ┆ 684408 │\n",
       "│ other-manufacturing           ┆ 4776   │\n",
       "└───────────────────────────────┴────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = ct.read_source_emissions(2022, \"/tmp\")\n",
    "(sdf\n",
    " .select(c_emissions_quantity.is_null().alias(\"null_emissions\"), c_subsector, c_iso3_country)\n",
    " .group_by(c_subsector, \"null_emissions\")\n",
    " .agg(pl.len())\n",
    " .collect()\n",
    " .pivot(index=SUBSECTOR, on=\"null_emissions\", values=\"len\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the Hugging Face Hub\n",
    "\n",
    "As a final step, we make the datasets available on Hugging Face as a downloadable dataset.\n",
    "\n",
    "This step will only work if you have the credentials to upload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate_trace-sources_v3-2024-ct3_2021.parquet /tmp/climate_trace-sources_v3-2024-ct3_2021.parquet\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid repo type, must be one of [None, 'model', 'dataset', 'space']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m         fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(fpath)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(fname, fpath)\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtjhunter/climate-trace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset-REMOVE_ME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mHfHubHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1286\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/climate-trace-nO4nxH9g-py3.10/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4364\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4252\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m \u001b[38;5;124;03mUpload a local file (up to 50 GB) to the given repo. The upload is done\u001b[39;00m\n\u001b[1;32m   4254\u001b[0m \u001b[38;5;124;03mthrough a HTTP post request, and doesn't require git or git-lfs to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4361\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   4362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m REPO_TYPES:\n\u001b[0;32m-> 4364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type, must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPO_TYPES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4366\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4367\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4368\u001b[0m )\n\u001b[1;32m   4369\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   4370\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   4371\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[1;32m   4372\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid repo type, must be one of [None, 'model', 'dataset', 'space']"
     ]
    }
   ],
   "source": [
    "import huggingface_hub.utils\n",
    "try:\n",
    "    api = huggingface_hub.HfApi()\n",
    "    for (_, fpath) in write_data():\n",
    "        fname = os.path.basename(fpath)\n",
    "        print(fname, fpath)\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=fname,\n",
    "            repo_id=\"tjhunter/climate-trace\",\n",
    "            repo_type=\"dataset-REMOVE_ME\",\n",
    "        )\n",
    "except huggingface_hub.utils.HfHubHTTPError as e:\n",
    "    print(\"error\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
